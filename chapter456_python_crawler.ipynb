{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四章 Urllib库与Error异常处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是urllib库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urllib是Python提供的一个用于操作URL的模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速使用Urllib爬取网页"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法1：urllib.request.urlopen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "{'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import chardet\n",
    "file = urllib.request.urlopen(\"http://www.baidu.com\")\n",
    "data = file.read()#是读取文件的全部内容，将读取到的内容赋值给字符串变量。type类型为bytes！！！\n",
    "#dataline = file.readline()#读取文件一行的内容\n",
    "#datalines = file.readlines()#读取文件全部的内容，和read()不同，readlines()会把读取到的内容赋值给一个列表变量。\n",
    "print(type(data))#显示data类型\n",
    "print(chardet.detect(data))#显示编码格式\n",
    "data = data.decode('utf-8')#转换类型bytes to str; str to bytes为str.encode(s)；\n",
    "print(type(data))#此时类型为str\n",
    "fhandle = open(\"C:/Users/yangyehong/Documents/Python Scripts/1.html\",\"w\",encoding='utf-8')\n",
    "#在windows下面，新文件的默认编码是gbk，这样的话，python解释器会用gbk编码去解析我们的网络数据流txt，\n",
    "#然而txt此时已经是decode过的unicode编码，这样的话就会导致解析不了，出现上述问题。 \n",
    "#解决的办法就是，改变目标文件的编码,定义utf-8\n",
    "fhandle.write(data)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题-乱码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#现在百度首页也可以了？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法2 urlretrieve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = urllib.request.urlretrieve(\"http://edu.51cto.com\",filename = \"C:/Users/yangyehong/Documents/Python Scripts/2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urllib.request.urlcleanup()#清楚这个函数造成的缓存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题-乱码-解决？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#中文乱码问题，如何改变编码。\n",
    "#现有方法：\"http://edu.51cto.com\"可用记事本另存为Unicode格式，2-2.html为不乱码。\n",
    "#www.baidu.com无法解决？？？？？百度可能有所设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 返回与当前环境有关的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPMessage at 0x1fd0e77b438>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.info()#返回与当前环境有关的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 当前爬取网页的状态码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.getcode()#当前爬取网页的状态码，200为正确，其他不对"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取当前爬取的url地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.baidu.com'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.geturl()#获取当前爬取的url地址\n",
    "#一般来说，URL标准中只会允许一部分ASCII字符比如数字、字母、部分符号等，其他字符如汉字等，是不符合URL标准的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL编码问题-解码&编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http%3A//www.sina.com.cn'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#所以，不合标准的字符就会出现问题，此时需要进行URL编码解决。urllib.request.quote()\n",
    "urllib.request.quote(\"http://www.sina.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.sina.com.cn'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#有时需要解码\n",
    "urllib.request.unquote(\"http%3A//www.sina.com.cn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浏览器的模拟——Headers属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "部分网页反爬虫设置，设置Headers信息，模拟成浏览器去访问这些网站。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://blog.csdn.net/weixin_pig/article/details/51178226\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file = urllib.request.urlopen(url)#出现错误403，禁止访问"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法1：buil_opener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "#url = \"https://www.jianshu.com/p/4133e8758488\"\n",
    "url = \"https://www.baidu.com\"\n",
    "headers = (\"User-Agent\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393\")\n",
    "#User-Agent获取方法：F12打开，然后在network中查看动作，选中网址，查看、复制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [headers]\n",
    "data = opener.open(url).read()\n",
    "fhand = open(\"C:/Users/yangyehong/Documents/Python Scripts/3.html\",\"wb\")\n",
    "#此处无乱码\n",
    "fhand.write(data)\n",
    "fhand.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法2：add_header()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://www.baidu.com\"\n",
    "#url = url.encode('utf-8')\n",
    "req = urllib.request.Request(url)#创建Request对象复制给变量req\n",
    "req.add_header('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158946\n",
      "{'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = urllib.request.urlopen(req).read()\n",
    "print(len(data))\n",
    "print(chardet.detect(data))#Python无法识别gbk\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = unicode(data,\"gb2312\").encode(\"utf8\")\n",
    "#data = data.encode('ISO-8559-1')\n",
    "#data = data.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fhand = open(\"C:/Users/yangyehong/Documents/Python Scripts/4.html\",\"w\",encoding='utf-8')\n",
    "fhand = open(\"C:/Users/yangyehong/Documents/Python Scripts/4.html\",\"wb\")\n",
    "fhand.write(data)\n",
    "fhand.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = data.decode('utf-8','ignore')#可以忽略gbk和utf8之间的矛盾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题-乱码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#又不乱码了，神奇\n",
    "from bs4 import BeautifulSoup\n",
    "#使用ISO-8859-1解码之后的代码\n",
    "soup = BeautifulSoup(html)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超时设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置网页请求的超时时间，有的快10秒，有些慢80秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#此处设置timeout的值为1，会在部分访问时候出现异常，将timeout设置较大一些，如30则会减少或避免异常\n",
    "for i in range(1,100):\n",
    "    try:\n",
    "        file = urllib.request.urlopen(\"http://yum.iqianyue.com\",timeout=1)\n",
    "        data = file.read()\n",
    "        print(len(data))\n",
    "    except Exception as e:\n",
    "        print(\"出现异常-->\"+str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP协议请求实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要进行客户端与服务器之间的消息传递，则可使用HTTP协议请求进行，分为6种类型：  \n",
    "（1）？？？GET请求：通过URL网址传递信息。可以直接在URL中写上要传递的信息，也可以由表单进行传递。如果使用表单进行传递，这表单中的信息会自动转为URL地址中的数据，通过URL地址传递。  \n",
    "（2）POST请求：可向服务器提交数据，比较主流且安全的数据传递方式。比如登录时，经常使用POST请求发送数据。  \n",
    "（3）DELETE请求：请求服务器删除一个资源  \n",
    "（4）HEAD请求：请求获取对应的HTTP报头信息。  \n",
    "（5）OPTIONS请求：可以获得当前URL所支持的请求类型。  \n",
    "（6）TRACE请求和CONNECT请求等  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get()搜索关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}\n",
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "#搜索关键词为英文\n",
    "keywd=\"hello\"\n",
    "url = \"http://www.baidu.com/s?wd=\"+keywd\n",
    "req = urllib.request.Request(url)\n",
    "data = urllib.request.urlopen(req).read()\n",
    "print(chardet.detect(data))#显示编码格式\n",
    "#data = data.decode('utf-8')#转换类型bytes to str; str to bytes为str.encode(s)；\n",
    "print(type(data))#此时类型为str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fhandle = open(\"C:/Users/yangyehong/Documents/Python Scripts/5.html\",\"wb\")\n",
    "fhandle.write(data)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#如果搜索关键词为中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"http://www.baidu.com/s?wd=\"\n",
    "key = \"韦玮老师\"\n",
    "key_code = urllib.request.quote(key)#对关键词进行编码，编码之后再构造为完整URL\n",
    "url_all = url+key_code\n",
    "req = urllib.request.Request(url_all)\n",
    "data = urllib.request.urlopen(req).read()\n",
    "fh = open(\"C:/Users/yangyehong/Documents/Python Scripts/6.html\",\"wb\")\n",
    "fh.write(data)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题-网页乱码-ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "又不乱码了\n",
    "维基百科没有出现乱码\n",
    "data = data.decode('utf-8','ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      ">>test\n",
      "encoding: utf8\n",
      "apparent_encoding: ascii\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://www.baidu.com\"\n",
    "response = requests.get(url)\n",
    "#if response.status_code!= 200:\n",
    "#    return None\n",
    "print (\"ok\")\n",
    "print (\">>test\")\n",
    "#输出response的网页内容编码和response的网页的头部的编码\n",
    "#response的网页内容编码\n",
    "response.encoding=('utf8')\n",
    "print ('encoding:',response.encoding)\n",
    "#response的网页头部的编码\n",
    "print ('apparent_encoding:',response.apparent_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = response.text\n",
    "fhand = open(\"C:/Users/yangyehong/Documents/Python Scripts/7.html\",\"w\",encoding='utf-8')\n",
    "fhand.write(data)\n",
    "fhand.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取、改变系统默认编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getdefaultencoding())\n",
    "#sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "url = \"https://www.oiegg.com/logging.php?action=login&\"\n",
    "postdata = urllib.parse.urlencode({\n",
    "    \"username\":\"茗默呵呵\",\n",
    "    \"password\":\"hon730516\"\n",
    "}).encode('utf-8')\n",
    "\n",
    "req = urllib.request.Request(url,postdata)#创建Request()对象\n",
    "req.add_header('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393')\n",
    "data = urllib.request.urlopen(req).read()\n",
    "fhandle = open(\"C:/Users/yangyehong/Documents/Python Scripts/9-dan.html\",\"wb\")\n",
    "fhandle.write(data)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好像有问题？？？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\r\n",
      "<head>\r\n",
      "<title>Post Test Page</title>\r\n",
      "</head>\r\n",
      "\r\n",
      "<body>\r\n",
      "<form action=\"\" method=\"post\">\r\n",
      "name:<input name=\"name\" type=\"text\" /><br>\r\n",
      "passwd:<input name=\"pass\" type=\"text\" /><br>\r\n",
      "<input name=\"\" type=\"submit\" value=\"submit\" />\r\n",
      "<br />\r\n",
      "you input name is:213161836<br>you input passwd is:15713927150yzxc</body>\r\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "url = \"http://www.iqianyue.com/mypost\"\n",
    "\n",
    "postdata = urllib.parse.urlencode({\n",
    "    \"name\":\"213161836\",\n",
    "    \"pass\":\"15713927150yzxc\"\n",
    "\n",
    "}).encode(\"utf-8\") #将数据使用urlencode编码后，使用encode（）设置utf-8编码\n",
    "\n",
    "req = urllib.request.Request(url,postdata)\n",
    "req.add_header(\"User-Agent\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safri/537.36\")\n",
    "data = urllib.request.urlopen(req).read().decode(\"utf-8\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代理服务器的设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "防止同一IP被禁止访问，使用代理服务器不显示我们真实的IP地址。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155305\n"
     ]
    }
   ],
   "source": [
    "def use_proxy(proxy_addr,url):\n",
    "    import urllib.request\n",
    "    proxy = urllib.request.ProxyHandler({'http':proxy_addr})\n",
    "    opener = urllib.request.build_opener(proxy,urllib.request.HTTPHandler)#定义一个opener\n",
    "    urllib.request.install_opener(opener)\n",
    "    data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    return data\n",
    "proxy_addr = \"183.245.99.52:80\"#代理地址-搜索“国内高匿代理IP\"\n",
    "data = use_proxy(proxy_addr,\"http://www.baidu.com\")\n",
    "print(len(data))\n",
    "#如果出现错误“ 由于目标计算机积极拒绝，无法连接。> ”，则换个代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fhandle = open(\"C:/Users/yangyehong/Documents/Python Scripts/10.html\",\"w\",encoding = 'utf-8')\n",
    "fhandle.write(data)\n",
    "fhandle.close()\n",
    "#如果出现错误“'gbk' codec can't encode character '\\xbb' in position 29150: illegal multibyte sequence\",则缺少encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DebugLog实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有的时候，我们希望程序在运行的过程中，便运行边打印调试日志，此时需要开启，DebugLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send: b'GET / HTTP/1.1\\r\\nAccept-Encoding: identity\\r\\nHost: edu.51cto.com\\r\\nUser-Agent: Python-urllib/3.6\\r\\nConnection: close\\r\\n\\r\\n'\n",
      "reply: 'HTTP/1.1 200 OK\\r\\n'\n",
      "header: Date header: Content-Type header: Transfer-Encoding header: Connection header: Set-Cookie header: Server header: Vary header: Vary header: X-Powered-By header: Set-Cookie header: Set-Cookie header: Set-Cookie header: Load-Balancing header: Load-Balancing "
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "httphd = urllib.request.HTTPHandler(debuglevel=1)\n",
    "httpshd = urllib.request.HTTPSHandler(debuglevel=1)\n",
    "opener = urllib.request.build_opener(httphd,httpshd)#创建自定义的opener对象\n",
    "urllib.request.install_opener(opener)\n",
    "#创建全局默认的opener对象，这样使用urlopen()时，会使用我们安装的opener对象\n",
    "data = urllib.request.urlopen(\"http://edu.51cto.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异常处理神器——URLError实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "程序在执行的过程中会有异常，使用URLError类，我们首先要导入urllib.error模块，进行异常处理，  \n",
    "我们经常使用try…except语句，在try中执行主要代码，在except中捕获异常信息，并进行相应的异常处理。  \n",
    "此块主要介绍**URLError**和它的子类**HTTPError**  \n",
    "**1、URLError的原因**  \n",
    "（1）连接不上服务器  \n",
    "（2）远程URL不存在  \n",
    "（3）无网络  \n",
    "（4）触发了HTTPError  \n",
    "**HTTPError**不可以检测前三个错误  \n",
    "前三个错误没有e.code，最后一个会有e.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#URLError\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "try:\n",
    "    urllib.request.urlopen(\"http://blog.csdn.net\")\n",
    "except urllib.error.URLError as e:\n",
    "    print(e.code)\n",
    "    print(e.reason)\n",
    "#CSDN博客是禁止对文章进行爬取的，所以，如果没有模拟浏览器去爬取，必然会403，会引发except部分。\n",
    "#捕获异常信息e，给出异常状态和原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HTTPError\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "try:\n",
    "    urllib.request.urlopen(\"https://blog.csdn.net\")\n",
    "except urllib.error.HTTPError as e:\n",
    "    print(e.code)#输出状态码\n",
    "    print(e.reason)\n",
    "#CSDN博客是禁止对文章进行爬取的，所以，如果没有模拟浏览器去爬取，必然会403，会引发except部分。\n",
    "#捕获异常信息e，给出异常状态和原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**常见的状态码及含义**  \n",
    "200 OK 一切正常  \n",
    "301 moved permanently 重新定向到新的URL，永久性  \n",
    "302 Found 重新定向到新的URL，非永久性  \n",
    "304 Not Modified 请求的资源未更新  \n",
    "400 bad request 错误请求  \n",
    "401 unauthorized 未授权  \n",
    "403 forbidden 禁止  \n",
    "404 not found 未发现  \n",
    "500 Internal Server Error 服务器内部出现错误  \n",
    "501 Not Implemented 服务器不支持实现请求所需要的功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(1)优化1：由于我们不清楚到底是什么错误，先用子类进行处理，若无法处理，再用父类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 11001] getaddrinfo failed\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "try:\n",
    "    urllib.request.urlopen(\"http://blog.baidusss.net\")\n",
    "except urllib.error.HTTPError as e:\n",
    "    print(e.code)\n",
    "    print(e.reason)\n",
    "except urllib.error.URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）优化2：解决有的时候有e.code，有的时候没有，将两个Error合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 11001] getaddrinfo failed\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "try:\n",
    "    urllib.request.urlopen(\"http://blog.baidusss.net\")\n",
    "except urllib.error.URLError as e:\n",
    "    if hasattr(e,\"code\"):#判断是否有这个错误\n",
    "        print(e.code)\n",
    "    if hasattr(e,\"reason\"):\n",
    "        print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）一般来说，URL标准中只允许一部分ASCII字符，比如数字、字母、部分符号等，  \n",
    "而其他的一些字符，比如汉字等，是不符合URL标准的。所以，使用不符合标准的字符，如汉字，会出错。  \n",
    "需要进行URL编码解决。比如输入中文、“：”、”&“等不符合标准的字符时，需编码。\n",
    "\n",
    "（2）爬取出现403，是反爬虫设置。  \n",
    "\n",
    "（3）由于urlopen()不支持一些HTTP的高级功能，所以，我们需要修改报头，使用urllib.request.build_opener()\n",
    "\n",
    "（4）还可用urllib.request.Request()的add_header()实现浏览器模拟。\n",
    "\n",
    "（5）异常处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五章 正则表达式与Cookie的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "字符串处理，希望按照自定义的规则处理，将规则成为模式。  \n",
    "可用正则表达式来描述这些自定义规则，正则表达式也称为模式表达式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是正则表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正则表达式：**描述字符串排列的一套规则。主要用于字符串的匹配。  \n",
    "例如，提取一个网页中的所有电子邮件，总结电子邮件格式，写一个正则表达式，  \n",
    "提取满足该规则的字符串。  \n",
    "**Python中使用re模块实现正则表达式的功能**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则表达式基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几类：  \n",
    "1）普通字符  \n",
    "2）非打印字符  \n",
    "3）通用字符  \n",
    "4）原子表\n",
    "\n",
    "**（1）普通字符作为原子**  \n",
    "数字、大小写字母、下划线等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(5, 8), match='yue'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = 'yue'\n",
    "string = 'iqianyue'\n",
    "result1 = re.search(pattern,string)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(2)非打印字符作为原子**  \n",
    "所谓非打印字符，指的是一些字符串中用于格式控制的符号，比如换行符\\n、制表符\\t等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(23, 24), match='\\n'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = \"\\n\"\n",
    "string = '''http://yum.iqianyue.com\n",
    "http://baidu.com'''#换行\n",
    "result1 = re.search(pattern,string)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = \"\\n\"\n",
    "string = '''http://yum.iqianyue.comhttp://baidu.com'''#换行\n",
    "result1 = re.search(pattern,string)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)通用字符作为原子**  \n",
    "通用字符，即一个原子可以匹配一类字符。  \n",
    "\\w 匹配任意一个字母、数字或下划线  \n",
    "\\W 匹配除字母、数字和下划线以外的任意一个字符  \n",
    "\\d 匹配任意一个十进制数  \n",
    "\\D 匹配十进制数以外的任意一个其他字符  \n",
    "\\s 匹配任意一个空白字符  \n",
    "\\S 匹配除空白字符以外的任意一个其他字符  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(9, 18), match='45pythony'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = \"\\w\\dpython\\w\"\n",
    "string = \"abcdfphp345pythony_py\"\n",
    "result1 = re.search(pattern,string)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)原子表**  \n",
    "使用原子表，可以定义一组地位平等的原子，然后匹配的时候会取该原子表中的任意一个原子进行匹配，  \n",
    "在Python中，原子表由[]表示，如[xyz]就是一个原子表，这个3个原子地位平等。  \n",
    "如果定义正则表达式为“[xyz]py”，对应的源字符串是“xpython”，此时只要py前一位是x y z中的一个即可。  \n",
    "[^]代表的是除了中括号里面的原子均可以匹配，比如“[^xyz]py\"能匹配”apy\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(9, 19), match='45pythony_'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern1 = \"\\w\\dpython[xyz]\\w\"\n",
    "pattern2 = \"\\w\\dpython[^xyz]\\w\"\n",
    "pattern3 = \"\\w\\dpython[xyz]\\W\"\n",
    "string = \"abcdfphp345pythony_py\"\n",
    "result1 = re.search(pattern1,string)\n",
    "result2 = re.search(pattern2,string)\n",
    "result3 = re.search(pattern3,string)\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 元字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓元字符，指的是正则表达式中具有一些特殊含义的字符，比如重复N次前面的字符等。  \n",
    ". 匹配除换行符以外的任意字符  \n",
    "^ 匹配字符串的开始位置  \n",
    "$ 匹配字符串的结束位置  \n",
    "\\* 匹配0次、1次或多次前面的原子  \n",
    "? 匹配0次或1次前面的原子  \n",
    "\\+ 匹配1次或多次前面的原子  \n",
    "{n} 前面的原子恰好出现n次  \n",
    "{n,} 前面的原子至少出现n次  \n",
    "{n,m} 前面的原子至少出现n从，至多出现m次  \n",
    "| 模式选择符  \n",
    "() 模式单元符  \n",
    "\n",
    "**正则表达式的() [] {} 有着不同的意思**  \n",
    "（） 是为了提取匹配字符串的，表达式中有几个()就有几个相应的匹配字符串  \n",
    "(\\s*)表示连续空格的字符串  \n",
    "[] 是定义匹配的字符范围。比如[a-zA-Z0-9]表示相应位置的字符要匹配英文字符和数字。[\\s*表示空格或者*号]  \n",
    "{}一般是用来匹配的长度。比如\\s{3}表示匹配三个空格，\\s[1,3]表示匹配1到3个空格  \n",
    "(0-9)匹配'0-9'本身。[0-9]*匹配数字（注意后面有*，可以为空）[0-9]+匹配数字(注意后面有+，不可以为空)，  \n",
    "{0-9}写法是错误的  \n",
    "[0-9]{0,9}表示长度为0到9的数字字符串。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)任意匹配元字符**  \n",
    "\".python...\"匹配“Python”字符前面有1为，后边有3位格式的字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(10, 20), match='5pythony_p'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = \".python...\"\n",
    "string = \"abcdfphp345pythony_py\"\n",
    "result1 = re.search(pattern,string)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)边界限制字符**  \n",
    "“^\"匹配开始，”$\"匹配结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
      "<_sre.SRE_Match object; span=(19, 21), match='py'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern1 = \"^abd\"\n",
    "pattern2 = \"^abc\"\n",
    "pattern3 = \"py$\"\n",
    "pattern4 = \"ay$\"\n",
    "string = \"abcdfphp345pythony_py\"\n",
    "result1 = re.search(pattern1,string)\n",
    "result2 = re.search(pattern2,string)\n",
    "result3 = re.search(pattern3,string)\n",
    "result4 = re.search(pattern4,string)\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)限定符**  \n",
    "常用限定符包括\\*、?、\\+、{n}、{n,}、{n,m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(12, 18), match='python'>\n",
      "<_sre.SRE_Match object; span=(2, 5), match='cdd'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern1 = \"py.*n\" # *匹配0次、1次或多次前面的原子\n",
    "pattern2 = \"cd{2}\"\n",
    "pattern3 = \"cd{3}\"\n",
    "paatern4 = \"cd{2,}\"\n",
    "string = \"abcddfphp345pythony_py\"\n",
    "result1 = re.search(pattern1,string)\n",
    "result2 = re.search(pattern2,string)\n",
    "result3 = re.search(pattern3,string)\n",
    "result4 = re.search(pattern4,string)\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)模式选择符**  \n",
    "讲解模式选择符“|”，使用模式选择符，可设置多个模式，匹配时，可从中选择任意一个模式匹配。  \n",
    "比如正则表达式“python|php”，“python”和“php”均满足匹配条件。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(5, 8), match='php'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = \"python|php\"\n",
    "string = \"abcdfphp345pythony_py\"\n",
    "result1 = re.search(pattern,string)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)模式单元符**  \n",
    "使用\"()\"将一些原子组合成一个大原子使用，小括号里边的部分会被当做一个整体去使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(2, 4), match='cd'>\n",
      "<_sre.SRE_Match object; span=(2, 5), match='cdd'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern1 = \"(cd){1,}\"\n",
    "pattern2 = \"cd{1,}\"\n",
    "string = \"abcddfphp345pythony_py\"\n",
    "result1 = re.search(pattern1,string)\n",
    "result2 = re.search(pattern2,string)\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模式修正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓模式修正符，即可以在不改变正则表达式的情况下，通过模式修正符改变正则表达式的含义，从而实现一些匹配结果的调整等功能。  \n",
    "比如，可以使用模式修正符I让对于模式在匹配时不区分大小写。  \n",
    "+ I 匹配时忽略大小写  \n",
    "M 多行匹配  \n",
    "L 做本地化识别匹配  \n",
    "U 根据Unicode字符及解析字符  \n",
    "S 让.匹配包括换行符，即用了该模式修正后，\".\"匹配就可以匹配任意的字符了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<_sre.SRE_Match object; span=(12, 18), match='Python'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern1 = \"python\"\n",
    "pattern2 = \"python\"\n",
    "string = \"abcddfphp345Pythony_py\"\n",
    "result1 = re.search(pattern1,string)\n",
    "result2 = re.search(pattern2,string,re.I)\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贪婪模式与懒惰模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贪婪模式的核心点就是尽可能多地匹配，  \n",
    "懒惰模式的核心就是尽可能少地匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(6, 22), match='php345pythony_py'>\n",
      "<_sre.SRE_Match object; span=(6, 14), match='php345py'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern1 = \"p.*y\"#贪婪模式\n",
    "pattern2 = \"p.*?y\"#懒惰模式\n",
    "string = \"abcddfphp345pythony_py\"\n",
    "result1 = re.search(pattern1,string)\n",
    "result2 = re.search(pattern2,string)\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则表达式常见函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.match()函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.match(pattern,string,flag)#flag是可选参数，代表对应的标志位，可以放模式修正符等信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 8), match='apythonh'>\n",
      "(0, 8)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = \"apythonhellomypythonhispythonourpythonend\"\n",
    "pattern = \".python.\"\n",
    "result = re.match(pattern,string)\n",
    "result2 = re.match(pattern,string).span()\n",
    "print(result)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.search()函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此函数扫描整个字符串进行对应的匹配。与re.match()最大的不同是，  \n",
    "re.match()函数从源字符串开头进行匹配，re.search()函数会在全文中进行检索并匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<_sre.SRE_Match object; span=(6, 14), match='ypythonh'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = \"hellomypythonhispythonourpythonend\"\n",
    "pattern = \".python.\"\n",
    "result = re.match(pattern,string)#字符串开始位置不符合正则表达式，所以匹配不到\n",
    "result2 = re.search(pattern,string)\n",
    "print(result)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全局匹配函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在以上的匹配中，即便源字符串中有多个结果符合模式，也只会匹配一个结果，如何将符合模式的内容全部都匹配出来呢？  \n",
    "1）使用re.compile()对正则表达式进行预编译  \n",
    "2）编译后，使用findall()根据正则表达式从源字符串中将匹配的结果全部找出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ypythonh', 'spythono', 'rpythone']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = \"hellomypythonhispythonourpythonend\"\n",
    "pattern = re.compile(\".python.\")\n",
    "result = pattern.findall(string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ypythonh', 'spythono', 'rpythone']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = \"hellomypythonhispythonourpythonend\"\n",
    "pattern = \".python.\"\n",
    "result = re.compile(pattern).findall(string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ypythonh'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.sub()函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要替换某些字符串  \n",
    "re.sub(pattern,rep,string,max)#rep为要换成的字符串，第四个参数为可选项，代表最多替换的次数，如果忽略不懈，将全部替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellomphpiphpuphpnd\n",
      "hellomphpiphpurpythonend\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = \"hellomypythonhispythonourpythonend\"\n",
    "pattern = \".python.\"\n",
    "result = re.sub(pattern,\"php\",string)\n",
    "result2 = re.sub(pattern,\"php\",string,2)\n",
    "print(result)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见实例解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例1：匹配.com或.cn后缀的URL网址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(9, 29), match='http://www.baidu.com'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = \"[a-zA-Z]+://[^\\s]*[.com|.cn]\"#[^\\s]表示多个非空格的字符，\n",
    "string = \"<a href='http://www.baidu.com'>百度首页</a>\"\n",
    "result = re.search(pattern,string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例2：匹配电话号码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 12), match='021-67282636'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = \"\\d{4}-\\d{7}|\\d{3}-\\d{8}\"#匹配电话号码的正则表达式\n",
    "string = \"021-6728263682382265236\"\n",
    "result = re.search(pattern,string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例3：匹配电子邮件地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(59, 81), match='c-e+0@iqi-anyue.com.cn'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = \"\\w+([.+-]\\w+)*@\\w+([.-]\\w+)*\\.\\w+([.-]\\w+)*\"\n",
    "string = \"<a href='http://www.baidu.com'>百度首页</a><br><a href='mailto:c-e+0@iqi-anyue.com.cn'>电子邮件地址</a>\"\n",
    "result = re.search(pattern,string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是cookie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "访问互联网页面，通过HTTP协议进行，而HTTP协议是一个无状态协议，即无法维持会话之间的状态。（反复登录）  \n",
    "所以，我们需要将会话信息通过一些方式保存下来。常用方式有两种：  \n",
    "（1）Cookie。将所有会话信息保存在客户端，当我们访问同一个网站的其他页面的时候，会从cookie中读取对应的会话信息，从而判断目前会话的状态，比如可以判断是否已经登录等。  \n",
    "（2）Session。将会话信息保存在服务器端，但是服务器会给客户端发SessionID等信息，这些信息一般存在客户端的Cookie中，如果客户端禁止了Cookie，也会通过其他方式存储。  \n",
    "在爬取网页的时候，如果有了cookie，我们登录成功后，爬取该网站的其他网页时，则会保持登录状态进行内容的爬取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookiejar实战精析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "提交表单，F12，在控制台中寻找提交表单的正确url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "url = \"https://login.taobao.com/member/login.jhtml?redirectURL=https%3A%2F%2Fi.taobao.com%2Fmy_taobao.htm%3Fspm%3Da2156.1676643.a2226mz.3.2906195adAMdcu\"\n",
    "postdata = urllib.parse.urlencode({\n",
    "    \"TPL_username\":\"17888828903\",\n",
    "    \"TPL_password\":\"hon730\"\n",
    "}).encode('utf-8')\n",
    "\n",
    "req = urllib.request.Request(url,postdata)\n",
    "req.add_header('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393')\n",
    "data = urllib.request.urlopen(req).read()\n",
    "fhandle = open(\"C:/Users/yangyehong/Documents/Python Scripts/5_6.html\",\"wb\")\n",
    "fhandle.write(data)\n",
    "fhandle.close()\n",
    "url2=\"https://item.taobao.com/item.htm?spm=a21bo.2017.201867-rmds-0.1.595211d9brO8c7&scm=1007.12807.84406.100200300000004&id=544306297321&pvid=f9e28bd2-c104-44a9-8e6a-090ce6c62cc1\"\n",
    "req2=urllib.request.Request(url2,postdata)\n",
    "req2.add_header('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393')\n",
    "data2=urllib.request.urlopen(req2).read()#爬取该网站下其他网页\n",
    "fhandle = open(\"C:/Users/yangyehong/Documents/Python Scripts/5_6-2.html\",\"wb\")\n",
    "fhandle.write(data2)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#为啥还是未登录状态？？？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述，首页应该是登录状态，里边的页面应该是未登录状态  \n",
    "1）导入Cookie处理模块  \n",
    "2）使用http.cookiejar.CookieJar()创建CookieJar对象  \n",
    "3）使用HTTPCookieProcessor创建cookie处理器，并以其为参数构建opener对象  \n",
    "4）创建全局默认的opener对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import http.cookiejar\n",
    "url = \"https://login.taobao.com/member/login.jhtml?redirectURL=https%3A%2F%2Fi.taobao.com%2Fmy_taobao.htm%3Fspm%3Da2156.1676643.a2226mz.3.2906195adAMdcu\"\n",
    "postdata = urllib.parse.urlencode({\n",
    "    \"TPL_username\":\"17888828903\",\n",
    "    \"TPL_password\":\"hon730\"\n",
    "}).encode('utf-8')\n",
    "\n",
    "req = urllib.request.Request(url,postdata)\n",
    "req.add_header('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393')\n",
    "#使用http.cookiejar.CookieJar()创建CookieJar对象\n",
    "cjar=http.cookiejar.CookieJar()\n",
    "#使用HTTPCookieProcessor创建cookie处理器，并以其为参数构建opener对象\n",
    "opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cjar))\n",
    "#以opener安装为全局\n",
    "urllib.request.install_opener(opener)\n",
    "file = opener.open(req)\n",
    "data = file.read()\n",
    "file = open(\"C:/Users/yangyehong/Documents/Python Scripts/5_6-3.html\",\"wb\")\n",
    "file.write(data)\n",
    "file.close()\n",
    "url2=\"https://item.taobao.com/item.htm?spm=a21bo.2017.201867-rmds-0.1.595211d9brO8c7&scm=1007.12807.84406.100200300000004&id=544306297321&pvid=f9e28bd2-c104-44a9-8e6a-090ce6c62cc1\"\n",
    "data2=urllib.request.urlopen(url2).read()\n",
    "fhandle = open(\"C:/Users/yangyehong/Documents/Python Scripts/5_6-4.html\",\"wb\")\n",
    "fhandle.write(data2)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 问题-post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "好像没有什么用？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）有时我们在进行字符串处理的时候，希望按自定义的规则进行处理，这些规则即为模式。正则表达式可用来描述这些规则，正则表达式也称为模式表达式。  \n",
    "2）re模块实现python正则表达式功能。  \n",
    "3）正则表达式中常见原子：普通字符、非打印字符、通用字符作为原子、原子表。  \n",
    "4）模式修正符，不改变正则表达式的情况下，通过模式修正符改变正则表达式含义，从而实现一些匹配结果的调整功能。  \n",
    "5）HTTP协议用于访问网页，是一个无状态协议，无法维持会话之间的状态。  \n",
    "6）会话信息控制常用两种方式：通过Cookie保存会话信息、通过Session保存会话信息。  \n",
    "7）如果是Session保存会话信息，会话信息保存在会话信息，但是服务器给客户端发送的SessionID等信息一般会存在Cookie中。如果客户端禁止了Cookie，也会通过其它方式存储。大部分情况是，存到Cookie中。  \n",
    "8）python3处理Cookie，使用cookiejar库进行处理。  \n",
    "9）获得真实的登录地址，需进行分析，分析方法主要两种：1>F12调出界面，2>工具软件-Fiddler。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第六章 手写Python爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图片爬虫实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本思想：  \n",
    "1）建立一个爬取图片的函数，爬取过程为：先通过urllib.request.urlopen(url).read()读取全部代码，然后根据上面的第一个正则表达式过滤无关信息,剩下全部为图片的信息。  \n",
    "根据上面的第二个正则表达式进行第二次信息过滤，提取出该网页上所有的目标图片的链接，并将这些链接地址存储的一个列表中，随后遍历该列表，分别将对应链接通过urllib.request.urlretrieve(imageurl,filename=imagename)存储到本地，为了避免程序中途异常崩溃，我们可以建立异常处理。  \n",
    "2）通过for循环将该分类下的所有网页都爬取一遍，链接可以构造为url='https://list.jd.com/list.html?cat=1713,3287,3797&page=' + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "第一个正则表达式：<div id=\"plist\".+? <div class=\"page clearfix\">  \n",
    "#注意下方表达式和F12所看的源代码不同，应使用爬取的网页代码中的格式\n",
    "第二个正则表达式：<img width=\"200\" height=\"200\" data-img=\"1\" src=\"//(.+?\\.jpg)\">|<img width=\"200\" height=\"200\" data-img=\"1\" data-lazy-img=\"//(.+?\\.jpg)\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始爬取第1页第1张图片\n",
      "成功保存第1页第1张图片\n",
      "开始爬取第1页第2张图片\n",
      "成功保存第1页第2张图片\n",
      "开始爬取第1页第3张图片\n",
      "成功保存第1页第3张图片\n",
      "开始爬取第1页第4张图片\n",
      "成功保存第1页第4张图片\n",
      "开始爬取第1页第5张图片\n",
      "成功保存第1页第5张图片\n",
      "开始爬取第1页第6张图片\n",
      "成功保存第1页第6张图片\n",
      "开始爬取第1页第7张图片\n",
      "成功保存第1页第7张图片\n",
      "开始爬取第1页第8张图片\n",
      "成功保存第1页第8张图片\n",
      "开始爬取第1页第9张图片\n",
      "成功保存第1页第9张图片\n",
      "开始爬取第1页第10张图片\n",
      "成功保存第1页第10张图片\n",
      "开始爬取第1页第11张图片\n",
      "成功保存第1页第11张图片\n",
      "开始爬取第1页第12张图片\n",
      "成功保存第1页第12张图片\n",
      "开始爬取第1页第13张图片\n",
      "成功保存第1页第13张图片\n",
      "开始爬取第1页第14张图片\n",
      "成功保存第1页第14张图片\n",
      "开始爬取第1页第15张图片\n",
      "成功保存第1页第15张图片\n",
      "开始爬取第1页第16张图片\n",
      "成功保存第1页第16张图片\n",
      "开始爬取第1页第17张图片\n",
      "成功保存第1页第17张图片\n",
      "开始爬取第1页第18张图片\n",
      "成功保存第1页第18张图片\n",
      "开始爬取第1页第19张图片\n",
      "成功保存第1页第19张图片\n",
      "开始爬取第1页第20张图片\n",
      "成功保存第1页第20张图片\n",
      "开始爬取第1页第21张图片\n",
      "成功保存第1页第21张图片\n",
      "开始爬取第1页第22张图片\n",
      "成功保存第1页第22张图片\n",
      "开始爬取第1页第23张图片\n",
      "成功保存第1页第23张图片\n",
      "开始爬取第1页第24张图片\n",
      "成功保存第1页第24张图片\n",
      "开始爬取第1页第25张图片\n",
      "成功保存第1页第25张图片\n",
      "开始爬取第1页第26张图片\n",
      "成功保存第1页第26张图片\n",
      "开始爬取第1页第27张图片\n",
      "成功保存第1页第27张图片\n",
      "开始爬取第1页第28张图片\n",
      "成功保存第1页第28张图片\n",
      "开始爬取第1页第29张图片\n",
      "成功保存第1页第29张图片\n",
      "开始爬取第1页第30张图片\n",
      "成功保存第1页第30张图片\n",
      "开始爬取第1页第31张图片\n",
      "成功保存第1页第31张图片\n",
      "开始爬取第1页第32张图片\n",
      "成功保存第1页第32张图片\n",
      "开始爬取第1页第33张图片\n",
      "成功保存第1页第33张图片\n",
      "开始爬取第1页第34张图片\n",
      "成功保存第1页第34张图片\n",
      "开始爬取第1页第35张图片\n",
      "成功保存第1页第35张图片\n",
      "开始爬取第1页第36张图片\n",
      "成功保存第1页第36张图片\n",
      "开始爬取第1页第37张图片\n",
      "成功保存第1页第37张图片\n",
      "开始爬取第1页第38张图片\n",
      "成功保存第1页第38张图片\n",
      "开始爬取第1页第39张图片\n",
      "成功保存第1页第39张图片\n",
      "开始爬取第1页第40张图片\n",
      "成功保存第1页第40张图片\n",
      "开始爬取第1页第41张图片\n",
      "成功保存第1页第41张图片\n",
      "开始爬取第1页第42张图片\n",
      "成功保存第1页第42张图片\n",
      "开始爬取第1页第43张图片\n",
      "成功保存第1页第43张图片\n",
      "开始爬取第1页第44张图片\n",
      "成功保存第1页第44张图片\n",
      "开始爬取第1页第45张图片\n",
      "成功保存第1页第45张图片\n",
      "开始爬取第1页第46张图片\n",
      "成功保存第1页第46张图片\n",
      "开始爬取第1页第47张图片\n",
      "成功保存第1页第47张图片\n",
      "开始爬取第1页第48张图片\n",
      "成功保存第1页第48张图片\n",
      "开始爬取第1页第49张图片\n",
      "成功保存第1页第49张图片\n",
      "开始爬取第1页第50张图片\n",
      "成功保存第1页第50张图片\n",
      "开始爬取第1页第51张图片\n",
      "成功保存第1页第51张图片\n",
      "开始爬取第1页第52张图片\n",
      "成功保存第1页第52张图片\n",
      "开始爬取第1页第53张图片\n",
      "成功保存第1页第53张图片\n",
      "开始爬取第1页第54张图片\n",
      "成功保存第1页第54张图片\n",
      "开始爬取第1页第55张图片\n",
      "成功保存第1页第55张图片\n",
      "开始爬取第1页第56张图片\n",
      "成功保存第1页第56张图片\n",
      "开始爬取第1页第57张图片\n",
      "成功保存第1页第57张图片\n",
      "开始爬取第1页第58张图片\n",
      "成功保存第1页第58张图片\n",
      "开始爬取第1页第59张图片\n",
      "成功保存第1页第59张图片\n",
      "开始爬取第1页第60张图片\n",
      "成功保存第1页第60张图片\n",
      "开始爬取第1页第61张图片\n",
      "成功保存第1页第61张图片\n",
      "开始爬取第1页第62张图片\n",
      "成功保存第1页第62张图片\n",
      "开始爬取第1页第63张图片\n",
      "成功保存第1页第63张图片\n",
      "开始爬取第1页第64张图片\n",
      "成功保存第1页第64张图片\n",
      "开始爬取第1页第65张图片\n",
      "成功保存第1页第65张图片\n",
      "开始爬取第1页第66张图片\n",
      "成功保存第1页第66张图片\n",
      "开始爬取第1页第67张图片\n",
      "成功保存第1页第67张图片\n",
      "开始爬取第1页第68张图片\n",
      "成功保存第1页第68张图片\n",
      "开始爬取第1页第69张图片\n",
      "成功保存第1页第69张图片\n",
      "开始爬取第1页第70张图片\n",
      "成功保存第1页第70张图片\n",
      "开始爬取第1页第71张图片\n",
      "成功保存第1页第71张图片\n",
      "开始爬取第1页第72张图片\n",
      "成功保存第1页第72张图片\n",
      "开始爬取第1页第73张图片\n",
      "成功保存第1页第73张图片\n",
      "开始爬取第1页第74张图片\n",
      "成功保存第1页第74张图片\n",
      "开始爬取第1页第75张图片\n",
      "成功保存第1页第75张图片\n",
      "开始爬取第1页第76张图片\n",
      "成功保存第1页第76张图片\n",
      "开始爬取第1页第77张图片\n",
      "成功保存第1页第77张图片\n",
      "开始爬取第1页第78张图片\n",
      "成功保存第1页第78张图片\n",
      "开始爬取第1页第79张图片\n",
      "成功保存第1页第79张图片\n",
      "开始爬取第1页第80张图片\n",
      "成功保存第1页第80张图片\n",
      "开始爬取第1页第81张图片\n",
      "成功保存第1页第81张图片\n",
      "开始爬取第1页第82张图片\n",
      "成功保存第1页第82张图片\n",
      "开始爬取第1页第83张图片\n",
      "成功保存第1页第83张图片\n",
      "开始爬取第1页第84张图片\n",
      "成功保存第1页第84张图片\n",
      "开始爬取第2页第1张图片\n",
      "成功保存第2页第1张图片\n",
      "开始爬取第2页第2张图片\n",
      "成功保存第2页第2张图片\n",
      "开始爬取第2页第3张图片\n",
      "成功保存第2页第3张图片\n",
      "开始爬取第2页第4张图片\n",
      "成功保存第2页第4张图片\n",
      "开始爬取第2页第5张图片\n",
      "成功保存第2页第5张图片\n",
      "开始爬取第2页第6张图片\n",
      "成功保存第2页第6张图片\n",
      "开始爬取第2页第7张图片\n",
      "成功保存第2页第7张图片\n",
      "开始爬取第2页第8张图片\n",
      "成功保存第2页第8张图片\n",
      "开始爬取第2页第9张图片\n",
      "成功保存第2页第9张图片\n",
      "开始爬取第2页第10张图片\n",
      "成功保存第2页第10张图片\n",
      "开始爬取第2页第11张图片\n",
      "成功保存第2页第11张图片\n",
      "开始爬取第2页第12张图片\n",
      "成功保存第2页第12张图片\n",
      "开始爬取第2页第13张图片\n",
      "成功保存第2页第13张图片\n",
      "开始爬取第2页第14张图片\n",
      "成功保存第2页第14张图片\n",
      "开始爬取第2页第15张图片\n",
      "成功保存第2页第15张图片\n",
      "开始爬取第2页第16张图片\n",
      "成功保存第2页第16张图片\n",
      "开始爬取第2页第17张图片\n",
      "成功保存第2页第17张图片\n",
      "开始爬取第2页第18张图片\n",
      "成功保存第2页第18张图片\n",
      "开始爬取第2页第19张图片\n",
      "成功保存第2页第19张图片\n",
      "开始爬取第2页第20张图片\n",
      "成功保存第2页第20张图片\n",
      "开始爬取第2页第21张图片\n",
      "成功保存第2页第21张图片\n",
      "开始爬取第2页第22张图片\n",
      "成功保存第2页第22张图片\n",
      "开始爬取第2页第23张图片\n",
      "成功保存第2页第23张图片\n",
      "开始爬取第2页第24张图片\n",
      "成功保存第2页第24张图片\n",
      "开始爬取第2页第25张图片\n",
      "成功保存第2页第25张图片\n",
      "开始爬取第2页第26张图片\n",
      "成功保存第2页第26张图片\n",
      "开始爬取第2页第27张图片\n",
      "成功保存第2页第27张图片\n",
      "开始爬取第2页第28张图片\n",
      "成功保存第2页第28张图片\n",
      "开始爬取第2页第29张图片\n",
      "成功保存第2页第29张图片\n",
      "开始爬取第2页第30张图片\n",
      "成功保存第2页第30张图片\n",
      "开始爬取第2页第31张图片\n",
      "成功保存第2页第31张图片\n",
      "开始爬取第2页第32张图片\n",
      "成功保存第2页第32张图片\n",
      "开始爬取第2页第33张图片\n",
      "成功保存第2页第33张图片\n",
      "开始爬取第2页第34张图片\n",
      "成功保存第2页第34张图片\n",
      "开始爬取第2页第35张图片\n",
      "成功保存第2页第35张图片\n",
      "开始爬取第2页第36张图片\n",
      "成功保存第2页第36张图片\n",
      "开始爬取第2页第37张图片\n",
      "成功保存第2页第37张图片\n",
      "开始爬取第2页第38张图片\n",
      "成功保存第2页第38张图片\n",
      "开始爬取第2页第39张图片\n",
      "成功保存第2页第39张图片\n",
      "开始爬取第2页第40张图片\n",
      "成功保存第2页第40张图片\n",
      "开始爬取第2页第41张图片\n",
      "成功保存第2页第41张图片\n",
      "开始爬取第2页第42张图片\n",
      "成功保存第2页第42张图片\n",
      "开始爬取第2页第43张图片\n",
      "成功保存第2页第43张图片\n",
      "开始爬取第2页第44张图片\n",
      "成功保存第2页第44张图片\n",
      "开始爬取第2页第45张图片\n",
      "成功保存第2页第45张图片\n",
      "开始爬取第2页第46张图片\n",
      "成功保存第2页第46张图片\n",
      "开始爬取第2页第47张图片\n",
      "成功保存第2页第47张图片\n",
      "开始爬取第2页第48张图片\n",
      "成功保存第2页第48张图片\n",
      "开始爬取第2页第49张图片\n",
      "成功保存第2页第49张图片\n",
      "开始爬取第2页第50张图片\n",
      "成功保存第2页第50张图片\n",
      "开始爬取第2页第51张图片\n",
      "成功保存第2页第51张图片\n",
      "开始爬取第2页第52张图片\n",
      "成功保存第2页第52张图片\n",
      "开始爬取第2页第53张图片\n",
      "成功保存第2页第53张图片\n",
      "开始爬取第2页第54张图片\n",
      "成功保存第2页第54张图片\n",
      "开始爬取第2页第55张图片\n",
      "成功保存第2页第55张图片\n",
      "开始爬取第2页第56张图片\n",
      "成功保存第2页第56张图片\n",
      "开始爬取第2页第57张图片\n",
      "成功保存第2页第57张图片\n",
      "开始爬取第2页第58张图片\n",
      "成功保存第2页第58张图片\n",
      "开始爬取第2页第59张图片\n",
      "成功保存第2页第59张图片\n",
      "开始爬取第2页第60张图片\n",
      "成功保存第2页第60张图片\n",
      "开始爬取第2页第61张图片\n",
      "成功保存第2页第61张图片\n",
      "开始爬取第2页第62张图片\n",
      "成功保存第2页第62张图片\n",
      "开始爬取第2页第63张图片\n",
      "成功保存第2页第63张图片\n",
      "开始爬取第2页第64张图片\n",
      "成功保存第2页第64张图片\n",
      "开始爬取第2页第65张图片\n",
      "成功保存第2页第65张图片\n",
      "开始爬取第2页第66张图片\n",
      "成功保存第2页第66张图片\n",
      "开始爬取第2页第67张图片\n",
      "成功保存第2页第67张图片\n",
      "开始爬取第2页第68张图片\n",
      "成功保存第2页第68张图片\n",
      "开始爬取第2页第69张图片\n",
      "成功保存第2页第69张图片\n",
      "开始爬取第2页第70张图片\n",
      "成功保存第2页第70张图片\n",
      "开始爬取第2页第71张图片\n",
      "成功保存第2页第71张图片\n",
      "爬取图片结束，成功保存155张图\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "\n",
    "sum = 0\n",
    "def craw(url,page):\n",
    "    html1=urllib.request.urlopen(url).read()\n",
    "    html1=str(html1)\n",
    "    pat1=r'<div id=\"plist\".+? <div class=\"page clearfix\">'\n",
    "    result1=re.compile(pat1).findall(html1)#全局匹配\n",
    "    result1=result1[0]\n",
    "    pat2=r'<img width=\"200\" height=\"200\" data-img=\"1\" src=\"//(.+?\\.jpg)\">|<img width=\"200\" height=\"200\" data-img=\"1\" data-lazy-img=\"//(.+?\\.jpg)\">'\n",
    "    imagelist=re.compile(pat2).findall(result1)\n",
    "    x=1\n",
    "    global sum\n",
    "    for imageurl in imagelist:\n",
    "        imagename='d:/BaiduNetdiskDownload/'+str(page)+str(x)+'.jpg'\n",
    "        if imageurl[0]!='':\n",
    "            imageurl='http://'+imageurl[0]\n",
    "        else:\n",
    "            imageurl='http://'+imageurl[1]\n",
    "        print('开始爬取第%d页第%d张图片'%(page,x))\n",
    "\n",
    "        try:\n",
    "            urllib.request.urlretrieve(imageurl,filename=imagename)\n",
    "        except urllib.error.URLError as e:\n",
    "            if hasattr(e,'code') or hasattr(e,'reason'):\n",
    "                x+=1\n",
    "\n",
    "        print('成功保存第%d页第%d张图片'%(page,x))\n",
    "        x+=1\n",
    "        sum+=1\n",
    "\n",
    "for i in range(1,3):\n",
    "    url='https://list.jd.com/list.html?cat=1713,3287,3797&page='+str(i)\n",
    "    craw(url,i)\n",
    "print('爬取图片结束，成功保存%d张图'%sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "开始爬取第1页第1张图片\n",
      "成功保存第1页第1张图片\n",
      "开始爬取第1页第2张图片\n",
      "成功保存第1页第2张图片\n",
      "开始爬取第1页第3张图片\n",
      "成功保存第1页第3张图片\n",
      "开始爬取第1页第4张图片\n",
      "成功保存第1页第4张图片\n",
      "开始爬取第1页第5张图片\n",
      "成功保存第1页第5张图片\n",
      "开始爬取第1页第6张图片\n",
      "成功保存第1页第6张图片\n",
      "开始爬取第1页第7张图片\n",
      "成功保存第1页第7张图片\n",
      "开始爬取第1页第8张图片\n",
      "成功保存第1页第8张图片\n",
      "开始爬取第1页第9张图片\n",
      "成功保存第1页第9张图片\n",
      "开始爬取第1页第10张图片\n",
      "成功保存第1页第10张图片\n",
      "开始爬取第1页第11张图片\n",
      "成功保存第1页第11张图片\n",
      "开始爬取第1页第12张图片\n",
      "成功保存第1页第12张图片\n",
      "开始爬取第1页第13张图片\n",
      "成功保存第1页第13张图片\n",
      "开始爬取第1页第14张图片\n",
      "成功保存第1页第14张图片\n",
      "开始爬取第1页第15张图片\n",
      "成功保存第1页第15张图片\n",
      "开始爬取第1页第16张图片\n",
      "成功保存第1页第16张图片\n",
      "开始爬取第1页第17张图片\n",
      "成功保存第1页第17张图片\n",
      "开始爬取第1页第18张图片\n",
      "成功保存第1页第18张图片\n",
      "开始爬取第1页第19张图片\n",
      "成功保存第1页第19张图片\n",
      "开始爬取第1页第20张图片\n",
      "成功保存第1页第20张图片\n",
      "开始爬取第1页第21张图片\n",
      "成功保存第1页第21张图片\n",
      "开始爬取第1页第22张图片\n",
      "成功保存第1页第22张图片\n",
      "开始爬取第1页第23张图片\n",
      "成功保存第1页第23张图片\n",
      "开始爬取第1页第24张图片\n",
      "成功保存第1页第24张图片\n",
      "开始爬取第1页第25张图片\n",
      "成功保存第1页第25张图片\n",
      "开始爬取第1页第26张图片\n",
      "成功保存第1页第26张图片\n",
      "开始爬取第1页第27张图片\n",
      "成功保存第1页第27张图片\n",
      "开始爬取第1页第28张图片\n",
      "成功保存第1页第28张图片\n",
      "开始爬取第1页第29张图片\n",
      "成功保存第1页第29张图片\n",
      "开始爬取第1页第30张图片\n",
      "成功保存第1页第30张图片\n",
      "开始爬取第1页第31张图片\n",
      "成功保存第1页第31张图片\n",
      "开始爬取第1页第32张图片\n",
      "成功保存第1页第32张图片\n",
      "开始爬取第1页第33张图片\n",
      "成功保存第1页第33张图片\n",
      "开始爬取第1页第34张图片\n",
      "成功保存第1页第34张图片\n",
      "开始爬取第1页第35张图片\n",
      "成功保存第1页第35张图片\n",
      "开始爬取第1页第36张图片\n",
      "成功保存第1页第36张图片\n",
      "开始爬取第1页第37张图片\n",
      "成功保存第1页第37张图片\n",
      "开始爬取第1页第38张图片\n",
      "成功保存第1页第38张图片\n",
      "开始爬取第1页第39张图片\n",
      "成功保存第1页第39张图片\n",
      "开始爬取第1页第40张图片\n",
      "成功保存第1页第40张图片\n",
      "开始爬取第1页第41张图片\n",
      "成功保存第1页第41张图片\n",
      "开始爬取第1页第42张图片\n",
      "成功保存第1页第42张图片\n",
      "开始爬取第1页第43张图片\n",
      "成功保存第1页第43张图片\n",
      "开始爬取第1页第44张图片\n",
      "成功保存第1页第44张图片\n",
      "开始爬取第1页第45张图片\n",
      "成功保存第1页第45张图片\n",
      "开始爬取第1页第46张图片\n",
      "成功保存第1页第46张图片\n",
      "开始爬取第1页第47张图片\n",
      "成功保存第1页第47张图片\n",
      "开始爬取第1页第48张图片\n",
      "成功保存第1页第48张图片\n",
      "开始爬取第1页第49张图片\n",
      "成功保存第1页第49张图片\n",
      "开始爬取第1页第50张图片\n",
      "成功保存第1页第50张图片\n",
      "开始爬取第1页第51张图片\n",
      "成功保存第1页第51张图片\n",
      "开始爬取第1页第52张图片\n",
      "成功保存第1页第52张图片\n",
      "开始爬取第1页第53张图片\n",
      "成功保存第1页第53张图片\n",
      "开始爬取第1页第54张图片\n",
      "成功保存第1页第54张图片\n",
      "开始爬取第1页第55张图片\n",
      "成功保存第1页第55张图片\n",
      "开始爬取第1页第56张图片\n",
      "成功保存第1页第56张图片\n",
      "开始爬取第1页第57张图片\n",
      "成功保存第1页第57张图片\n",
      "开始爬取第1页第58张图片\n",
      "成功保存第1页第58张图片\n",
      "开始爬取第1页第59张图片\n",
      "成功保存第1页第59张图片\n",
      "开始爬取第1页第60张图片\n",
      "成功保存第1页第60张图片\n",
      "开始爬取第1页第61张图片\n",
      "成功保存第1页第61张图片\n",
      "开始爬取第1页第62张图片\n",
      "成功保存第1页第62张图片\n",
      "开始爬取第1页第63张图片\n",
      "成功保存第1页第63张图片\n",
      "开始爬取第1页第64张图片\n",
      "成功保存第1页第64张图片\n",
      "开始爬取第1页第65张图片\n",
      "成功保存第1页第65张图片\n",
      "开始爬取第1页第66张图片\n",
      "成功保存第1页第66张图片\n",
      "开始爬取第1页第67张图片\n",
      "成功保存第1页第67张图片\n",
      "开始爬取第1页第68张图片\n",
      "成功保存第1页第68张图片\n",
      "开始爬取第1页第69张图片\n",
      "成功保存第1页第69张图片\n",
      "开始爬取第1页第70张图片\n",
      "成功保存第1页第70张图片\n",
      "开始爬取第1页第71张图片\n",
      "成功保存第1页第71张图片\n",
      "开始爬取第1页第72张图片\n",
      "成功保存第1页第72张图片\n",
      "开始爬取第1页第73张图片\n",
      "成功保存第1页第73张图片\n",
      "开始爬取第1页第74张图片\n",
      "成功保存第1页第74张图片\n",
      "开始爬取第1页第75张图片\n",
      "成功保存第1页第75张图片\n",
      "开始爬取第1页第76张图片\n",
      "成功保存第1页第76张图片\n",
      "开始爬取第1页第77张图片\n",
      "成功保存第1页第77张图片\n",
      "开始爬取第1页第78张图片\n",
      "成功保存第1页第78张图片\n",
      "开始爬取第1页第79张图片\n",
      "成功保存第1页第79张图片\n",
      "开始爬取第1页第80张图片\n",
      "成功保存第1页第80张图片\n",
      "开始爬取第1页第81张图片\n",
      "成功保存第1页第81张图片\n",
      "开始爬取第1页第82张图片\n",
      "成功保存第1页第82张图片\n",
      "开始爬取第1页第83张图片\n",
      "成功保存第1页第83张图片\n",
      "开始爬取第1页第84张图片\n",
      "成功保存第1页第84张图片\n",
      "71\n",
      "开始爬取第2页第1张图片\n",
      "成功保存第2页第1张图片\n",
      "开始爬取第2页第2张图片\n",
      "成功保存第2页第2张图片\n",
      "开始爬取第2页第3张图片\n",
      "成功保存第2页第3张图片\n",
      "开始爬取第2页第4张图片\n",
      "成功保存第2页第4张图片\n",
      "开始爬取第2页第5张图片\n",
      "成功保存第2页第5张图片\n",
      "开始爬取第2页第6张图片\n",
      "成功保存第2页第6张图片\n",
      "开始爬取第2页第7张图片\n",
      "成功保存第2页第7张图片\n",
      "开始爬取第2页第8张图片\n",
      "成功保存第2页第8张图片\n",
      "开始爬取第2页第9张图片\n",
      "成功保存第2页第9张图片\n",
      "开始爬取第2页第10张图片\n",
      "成功保存第2页第10张图片\n",
      "开始爬取第2页第11张图片\n",
      "成功保存第2页第11张图片\n",
      "开始爬取第2页第12张图片\n",
      "成功保存第2页第12张图片\n",
      "开始爬取第2页第13张图片\n",
      "成功保存第2页第13张图片\n",
      "开始爬取第2页第14张图片\n",
      "成功保存第2页第14张图片\n",
      "开始爬取第2页第15张图片\n",
      "成功保存第2页第15张图片\n",
      "开始爬取第2页第16张图片\n",
      "成功保存第2页第16张图片\n",
      "开始爬取第2页第17张图片\n",
      "成功保存第2页第17张图片\n",
      "开始爬取第2页第18张图片\n",
      "成功保存第2页第18张图片\n",
      "开始爬取第2页第19张图片\n",
      "成功保存第2页第19张图片\n",
      "开始爬取第2页第20张图片\n",
      "成功保存第2页第20张图片\n",
      "开始爬取第2页第21张图片\n",
      "成功保存第2页第21张图片\n",
      "开始爬取第2页第22张图片\n",
      "成功保存第2页第22张图片\n",
      "开始爬取第2页第23张图片\n",
      "成功保存第2页第23张图片\n",
      "开始爬取第2页第24张图片\n",
      "成功保存第2页第24张图片\n",
      "开始爬取第2页第25张图片\n",
      "成功保存第2页第25张图片\n",
      "开始爬取第2页第26张图片\n",
      "成功保存第2页第26张图片\n",
      "开始爬取第2页第27张图片\n",
      "成功保存第2页第27张图片\n",
      "开始爬取第2页第28张图片\n",
      "成功保存第2页第28张图片\n",
      "开始爬取第2页第29张图片\n",
      "成功保存第2页第29张图片\n",
      "开始爬取第2页第30张图片\n",
      "成功保存第2页第30张图片\n",
      "开始爬取第2页第31张图片\n",
      "成功保存第2页第31张图片\n",
      "开始爬取第2页第32张图片\n",
      "成功保存第2页第32张图片\n",
      "开始爬取第2页第33张图片\n",
      "成功保存第2页第33张图片\n",
      "开始爬取第2页第34张图片\n",
      "成功保存第2页第34张图片\n",
      "开始爬取第2页第35张图片\n",
      "成功保存第2页第35张图片\n",
      "开始爬取第2页第36张图片\n",
      "成功保存第2页第36张图片\n",
      "开始爬取第2页第37张图片\n",
      "成功保存第2页第37张图片\n",
      "开始爬取第2页第38张图片\n",
      "成功保存第2页第38张图片\n",
      "开始爬取第2页第39张图片\n",
      "成功保存第2页第39张图片\n",
      "开始爬取第2页第40张图片\n",
      "成功保存第2页第40张图片\n",
      "开始爬取第2页第41张图片\n",
      "成功保存第2页第41张图片\n",
      "开始爬取第2页第42张图片\n",
      "成功保存第2页第42张图片\n",
      "开始爬取第2页第43张图片\n",
      "成功保存第2页第43张图片\n",
      "开始爬取第2页第44张图片\n",
      "成功保存第2页第44张图片\n",
      "开始爬取第2页第45张图片\n",
      "成功保存第2页第45张图片\n",
      "开始爬取第2页第46张图片\n",
      "成功保存第2页第46张图片\n",
      "开始爬取第2页第47张图片\n",
      "成功保存第2页第47张图片\n",
      "开始爬取第2页第48张图片\n",
      "成功保存第2页第48张图片\n",
      "开始爬取第2页第49张图片\n",
      "成功保存第2页第49张图片\n",
      "开始爬取第2页第50张图片\n",
      "成功保存第2页第50张图片\n",
      "开始爬取第2页第51张图片\n",
      "成功保存第2页第51张图片\n",
      "开始爬取第2页第52张图片\n",
      "成功保存第2页第52张图片\n",
      "开始爬取第2页第53张图片\n",
      "成功保存第2页第53张图片\n",
      "开始爬取第2页第54张图片\n",
      "成功保存第2页第54张图片\n",
      "开始爬取第2页第55张图片\n",
      "成功保存第2页第55张图片\n",
      "开始爬取第2页第56张图片\n",
      "成功保存第2页第56张图片\n",
      "开始爬取第2页第57张图片\n",
      "成功保存第2页第57张图片\n",
      "开始爬取第2页第58张图片\n",
      "成功保存第2页第58张图片\n",
      "开始爬取第2页第59张图片\n",
      "成功保存第2页第59张图片\n",
      "开始爬取第2页第60张图片\n",
      "成功保存第2页第60张图片\n",
      "开始爬取第2页第61张图片\n",
      "成功保存第2页第61张图片\n",
      "开始爬取第2页第62张图片\n",
      "成功保存第2页第62张图片\n",
      "开始爬取第2页第63张图片\n",
      "成功保存第2页第63张图片\n",
      "开始爬取第2页第64张图片\n",
      "成功保存第2页第64张图片\n",
      "开始爬取第2页第65张图片\n",
      "成功保存第2页第65张图片\n",
      "开始爬取第2页第66张图片\n",
      "成功保存第2页第66张图片\n",
      "开始爬取第2页第67张图片\n",
      "成功保存第2页第67张图片\n",
      "开始爬取第2页第68张图片\n",
      "成功保存第2页第68张图片\n",
      "开始爬取第2页第69张图片\n",
      "成功保存第2页第69张图片\n",
      "开始爬取第2页第70张图片\n",
      "成功保存第2页第70张图片\n",
      "开始爬取第2页第71张图片\n",
      "成功保存第2页第71张图片\n",
      "爬取图片结束，成功保存155张图\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "\n",
    "sum = 0\n",
    "def craw(url,page):\n",
    "    html1=urllib.request.urlopen(url).read()\n",
    "    html1=str(html1)\n",
    "    pat1=r'<ul class=\"gl-warp clearfix\">.+?</ul>'\n",
    "    result1=re.compile(pat1).findall(html1)#全局匹配\n",
    "    result1=result1[0]#此处为什么？？？？\n",
    "    #print(result1)\n",
    "    pat2=r'<img width=\"200\" height=\"200\" data-img=\"1\" src=\"//(.+?\\.jpg)\">|<img width=\"200\" height=\"200\" data-img=\"1\" data-lazy-img=\"//(.+?\\.jpg)\">'\n",
    "    imagelist=re.compile(pat2).findall(result1)\n",
    "    print(len(imagelist))\n",
    "    imagelist1=str(imagelist)\n",
    "    file = open(\"d:/BaiduNetdiskDownload/1.txt\",\"w\")\n",
    "    file.write(imagelist1)\n",
    "    file.close()\n",
    "    x=1\n",
    "    global sum\n",
    "    for imageurl in imagelist:\n",
    "        imagename='d:/BaiduNetdiskDownload/'+str(page)+str(x)+'.jpg'\n",
    "        #重要的if-else代码\n",
    "        if imageurl[0]!='':\n",
    "            imageurl='http://'+imageurl[0]\n",
    "        else:\n",
    "            imageurl='http://'+imageurl[1]\n",
    "        print('开始爬取第%d页第%d张图片'%(page,x))\n",
    "        try:\n",
    "            urllib.request.urlretrieve(imageurl,filename=imagename)\n",
    "            print('成功保存第%d页第%d张图片'%(page,x))\n",
    "            sum+=1\n",
    "        except urllib.error.URLError as e:\n",
    "            if hasattr(e,'code') or hasattr(e,'reason'):\n",
    "                x+=1\n",
    "        x+=1\n",
    "        \n",
    "\n",
    "for i in range(1,3):\n",
    "    url='https://list.jd.com/list.html?cat=1713,3287,3797&page='+str(i)\n",
    "    craw(url,i)\n",
    "print('爬取图片结束，成功保存%d张图'%sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 链接爬虫实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "实现思路：  \n",
    "1）确定好要爬取的入口链接  \n",
    "2）根据需求构建好链接提取的正则表达式  \n",
    "3）模拟成浏览器并爬取对应网页  \n",
    "4）根据2）中的正则表达式提取出该网页中包含的链接  \n",
    "5）过滤掉重复的链接  \n",
    "6）后续操作。比如打印这些链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blog.csdn.net/Kaitiren/article/details/85066935\n",
      "https://blog.csdn.net/kangkanglhb88008\n",
      "https://csdnimg.cn/feed/20181212/0786a45d51381df4ae86a3aa785c673d.jpg\n",
      "https://avatar.csdn.net/4/A/8/1_kangkanglhb88008.jpg\n",
      "https://blog.csdn.net/wh_0727/article/details/84074266\n",
      "https://blog.csdn.net/HHTNAN\n",
      "https://blog.csdn.net/sunhuaqiang1/article/details/84991520\n",
      "https://csdnimg.cn/feed/20181011/8b06ac0790a5812087e0af0bc143baf2.png\n",
      "https://csdnimg.cn/feed/20181008/fd5673e4591d508ab7cac672d9e4031c.jpg\n",
      "https://csdnimg.cn/feed/20180913/aaa30b727dd7571fb9a71fbafb04f0de.jpg\n",
      "https://blog.csdn.net/dog250/article/details/82892267\n",
      "https://blog.csdn.net/beliefer/article/details/84998806\n",
      "https://csdnimg.cn/feed/20181008/64f9e39fd53627fb1b867775a7d6a754.jpg\n",
      "https://avatar.csdn.net/7/0/0/1_hhtnan.jpg\n",
      "https://blog.csdn.net/russell_tao/article/details/82377332\n",
      "https://csdnimg.cn/feed/20181212/614e91fd3d235087bcb9e9cc0112ad28.png\n",
      "https://blog.csdn.net/Java_3y/article/details/82107339\n",
      "https://blog.csdn.net/bit_kaki/article/details/82585718\n",
      "https://blog.csdn.net/weixin_43430036/article/details/84944372\n",
      "https://blog.csdn.net/yus201120/article/details/84073445\n",
      "https://mp.csdn.net/blogmove\n",
      "https://blockchain.csdn.net\n",
      "https://blog.csdn.net/turingbooks/article/details/82995901\n",
      "https://csdnimg.cn/feed/20181008/affb704967f5200cddf09dc2ffe8835a.jpg\n",
      "https://blog.csdn.net/qq_43685243/article/details/84073602\n",
      "https://blog.csdn.net/Jmilk\n",
      "https://blog.csdn.net/qq_43685243\n",
      "https://gitbook.cn/gitchat/column/5ad70dea9a722231b25ddbf8\n",
      "https://blog.csdn.net/fengbingchun/article/details/85030305\n",
      "https://csdnimg.cn/feed/20181217/99617bb3b33505cf9f03c3e06102dcdd.jpg\n",
      "https://blog.csdn.net/qq_34829447/article/details/85042697\n",
      "https://csdnimg.cn/feed/20181213/c062c89e428ac9185f1854ee1ea344bd.jpg\n",
      "https://blog.csdn.net/qq_43685588/article/details/84074160\n",
      "https://blog.csdn.net/wh_0727\n",
      "https://csdnimg.cn/feed/20181009/7f80d8ea9896099cf92ae677c414c182.png\n",
      "https://avatar.csdn.net/4/1/9/1_weixin_42325069.jpg\n",
      "https://csdnimg.cn/feed/20181217/2b530756eedc57f1e57271e1b0fa332c.png\n",
      "https://avatar.csdn.net/5/9/4/1_qq_43685588.jpg\n",
      "https://avatar.csdn.net/7/7/0/1_wh_0727.jpg\n",
      "https://csdnimg.cn/feed/20181218/b46fb2bb97e59d8fedfe1e3bf8e9f2b7.png\n",
      "https://gitbook.cn/gitchat/column/5ad56a79af8f2f35290f6535\n",
      "https://csdnimg.cn/feed/20181212/dc27662fe77eddc41c8b157a2e877b40.png\n",
      "https://blog.csdn.net/u012999985/article/details/80877671\n",
      "https://avatar.csdn.net/7/1/5/1_yus201120.jpg\n",
      "https://csdnimg.cn/feed/20181217/6e600a03da64bdd60ab485ec3d0f220b.png\n",
      "http://blog.csdn.net\n",
      "https://blog.csdn.net/kangkanglhb88008/article/details/84073780\n",
      "https://blog.csdn.net/qq_37667464/article/details/84074385\n",
      "https://csdnimg.cn/feed/20181219/3449afdb8ce16c537d77ca0e5fc023df.jpg\n",
      "https://blog.csdn.net/weixin_42325069/article/details/84074379\n",
      "https://blog.csdn.net/Trent1985/article/details/82686434\n",
      "https://blog.csdn.net/qq_37667464\n",
      "http://blog.csdn.net/experts/rule.html\n",
      "https://csdnimg.cn/feed/20181219/d122a78ef6080af6c94e31063ea6833a.jpg\n",
      "https://blog.csdn.net/silentwolfyh/article/details/82865579\n",
      "https://blog.csdn.net/gitchat/article/details/82971479\n",
      "https://blog.csdn.net/buptgshengod/article/details/85061059\n",
      "https://ads.csdn.net/js/async_new.js\n",
      "https://blog.csdn.net/\n",
      "http://gitbook.cn/gitchat/activity/5a52e91f5881a96df9f4c02c\\r\\ngitchat2:http://gitbook.cn/gi...\n",
      "https://csdnimg.cn/feed/20181008/5b7db67ced3b56af839f8b6d85d076a5.jpg\n",
      "https://blog.csdn.net/poem_qianmo/article/details/82731058\n",
      "https://blog.csdn.net/dog250/article/details/82812235\n",
      "https://csdnimg.cn/feed/20181217/34f4e4b478b98b796a4ab9d5b7024e1d.png\n",
      "https://csdnimg.cn/feed/20181217/59a269ce3eff8cccfd26ad9b661c8ca4.jpg\n",
      "https://blog.csdn.net/u010870518\n",
      "https://avatar.csdn.net/D/7/D/1_u010870518.jpg\n",
      "https://blog.csdn.net/weixin_42325069\n",
      "https://blog.csdn.net/wireless_com/article/details/85003784\n",
      "https://avatar.csdn.net/4/F/D/1_qq_37667464.jpg\n",
      "https://avatar.csdn.net/9/D/5/1_jmilk.jpg\n",
      "https://blog.csdn.net/liumiaocn/article/details/82696501\n",
      "https://csdnimg.cn/feed/20181008/e79279eca0fda46dbfaf319988130033.png\n",
      "https://csdnimg.cn/feed/20181218/4251c768c0e226f157024bfab37b80eb.png\n",
      "https://blog.csdn.net/qq_43685588\n",
      "https://blog.csdn.net/yus201120\n",
      "https://avatar.csdn.net/A/C/6/1_qq_43685243.jpg\n",
      "https://blog.csdn.net/oDaiLiDong/article/details/85037064\n",
      "https://download.csdn.net/download/k...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "\n",
    "def getlink(url):\n",
    "    # 模拟成浏览器\n",
    "    headers = (\"User-Agent\",\n",
    "               \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\")\n",
    "\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [headers]\n",
    "\n",
    "    # opener 为全局安装\n",
    "    urllib.request.install_opener(opener)\n",
    "    file = urllib.request.urlopen(url)\n",
    "    data = str(file.read())\n",
    "\n",
    "    # 根据要求构建好链接的表达式，基本格式为\"http://xxx.yy\"，其中“xxx\"和”yyy“均代表可变化部分。\n",
    "    #https中的s可有可无，然后xxx部分不可以出现空格、双引号、分号，yyy部分是一些非特殊字符或者/符号\n",
    "    pattern = '(https?://[^\\s)\";]+\\.(\\w|/)*)'\n",
    "    link = re.compile(pattern).findall(data)\n",
    "    # 去除重复元素????\n",
    "    link = list(set(link))\n",
    "    return link\n",
    "\n",
    "# 要爬取的网页链接\n",
    "url = \"http://blog.csdn.net/\"\n",
    "\n",
    "# 获取对应网页中包含的链接地址\n",
    "linklist = getlink(url)\n",
    "for link in linklist:\n",
    "    print(link[0], '\\t', link[1])#去掉后两个元素即可去掉后边的字母"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 糗事百科爬虫实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现思路：  \n",
    "1）分析页面的网址规律，构造网址变量，for循环实现多页内容爬取。  \n",
    "2）构建自定义函数。两部分：一为用户发表的段子，首先模拟浏览器访问，观察网页源代码中的内容，将用户信息部分与段子内容部分的格式写成正则表达式。然后，提取该页所有用户所有内容。随后，for循环遍历段子内容并将内容分别赋给对应变量，变量名有规律“content+顺序号”，接下来for循环遍历对应用户，并输出该用户对应的内容。  \n",
    "3）for循环分别获取多页的各页URL链接，每页分别调用一次getcontent(url,page)函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "def getcontent(url,page):\n",
    "    #模拟成浏览器\n",
    "    headers = (\"User-Agent\",\n",
    "               \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\")\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [headers]\n",
    "    #将opener安装为全局\n",
    "    urllib.request.install_opener(opener)\n",
    "    data = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "    #构建对应用户提取的正则表达式\n",
    "    userpat = '<span class=\"recmd-name\">(.*?)</span>'\n",
    "    #构建段子内容提取的正则表达式\n",
    "    contentpat = '<div class=\"content\">(.*?)</div>'\n",
    "    #寻找出所有用户\n",
    "    userlist=re.compile(userpat,re.S).findall(data)\n",
    "    contentlist=re.compile(contentpat,re.S)findall(data)\n",
    "    x=1\n",
    "    #通过for循环遍历段子内容并将内容分别赋给对应的变量\n",
    "    for centent in contentlist:\n",
    "        content=content.replace(\"\\n\",\"\")\n",
    "        #用字符串作为变量名，先将对应字符串赋给一个变量\n",
    "        name=\"content\"+str(x)\n",
    "        #通过exce()函数实现用字符串作为变量名并赋值\n",
    "        exec(name+'=content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "用户11是:米臭一白...\n",
      "内容是：\n",
      "像我这种单身🐶好像找到了幸福的生活！\n",
      "\n",
      "\n",
      "用户12是:留恋那抹...\n",
      "内容是：\n",
      "哈哈，铿锵有力，坚定不移，，\n",
      "\n",
      "\n",
      "用户13是:渣渣婉儿...\n",
      "内容是：\n",
      "你能怎样哈哈\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_51.png\" align=\"absmiddle\">\n",
      "\n",
      "\n",
      "用户14是:初夏～繁...\n",
      "内容是：\n",
      "老姐店里卖保.健.品的，今天经理派她出去发传单，她怕晒黑，全副武装起来，就露两眼睛，结果一天了也没拉一个顾客。想着会被经理责骂，老姐找我诉苦，我给她出了一个主意\n",
      "\n",
      "\n",
      "用户15是:南郭儿\n",
      "内容是：\n",
      "有一次上课时候老师让我们说一下自己的愿望，我当时就想愿望吗，肯定要一个好的嘛！等老师点明到我的时候，我还很自信的放大了声音。老师说:你的愿望是什么？我:第一呢，\n",
      "\n",
      "\n",
      "用户16是:嘿，请叫...\n",
      "内容是：\n",
      "大姨妈来了，肚子特别疼，打电话给男朋友让他拿个东西过来给我捂捂肚子，男朋友挂上电话就拿着一个保温杯火急火燎赶来了。如果不是捂半天都没把保温杯捂热，我想我还是挺感\n",
      "\n",
      "\n",
      "用户17是:打扰了啊...\n",
      "内容是：\n",
      "突然深情。\n",
      "\n",
      "\n",
      "用户18是:米娅伦旗...\n",
      "内容是：\n",
      "含着的？我很欣赏你的想法\n",
      "\n",
      "\n",
      "用户19是:逆蝶*\n",
      "内容是：\n",
      "这普通话一级标准\n",
      "\n",
      "\n",
      "用户110是:半根莲藕...\n",
      "内容是：\n",
      "你妈担心你嫁不出去是一种什么样的体验？\n",
      "\n",
      "\n",
      "用户111是:肖倾城\n",
      "内容是：\n",
      "王哥出差提前回家了，想着给媳妇一个惊喜，一手举着给媳妇买的礼物，一手打开了家门，谁知一股烟味扑鼻而来。王哥暗暗生疑:“哪来的烟味?&quot;只见媳妇随\n",
      "\n",
      "\n",
      "用户112是:窝里斗窝...\n",
      "内容是：\n",
      "宿舍里那些让所有人爆笑的情节！！\n",
      "\n",
      "\n",
      "用户113是:捻一丝江...\n",
      "内容是：\n",
      "最后一只霸王龙灭绝的原因\n",
      "\n",
      "\n",
      "用户114是:余生都会...\n",
      "内容是：\n",
      "劳资信了你的邪\n",
      "\n",
      "\n",
      "用户115是:Zee....\n",
      "内容是：\n",
      "不要惊扰大师…\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_18.png\" align=\"absmiddle\">\n",
      "\n",
      "\n",
      "用户116是:傻妞也\n",
      "内容是：\n",
      "孩子他爸假装晕倒，看孩子什么反应\n",
      "\n",
      "\n",
      "用户117是:鑫酱就酱...\n",
      "内容是：\n",
      "刚刚去洗内裤。拿了一条干净的，准备洗的还在身上。去洗手间手一扔把干净的扔进盆里加水。就开始洗，洗完了一晾，躺在床上突然感觉那里不对劲。🙃🙃🙃\n",
      "\n",
      "\n",
      "用户118是:嘿，请叫...\n",
      "内容是：\n",
      "男票在床上的哪个细节会让你性趣全无<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_00.png\" align=\"absmiddle\">​​​​\n",
      "\n",
      "\n",
      "用户119是:黎粤\n",
      "内容是：\n",
      "这似乎是真理\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_13.png\" align=\"absmiddle\">\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_13.png\" align=\"absmiddle\">\n",
      "\n",
      "\n",
      "用户120是:生末净旦...\n",
      "内容是：\n",
      "这些冷知识好像没毛病啊\n",
      "\n",
      "\n",
      "20\n",
      "用户21是:米臭一白...\n",
      "内容是：\n",
      "如此粉丝，实乃真粉真爱！太牛了\n",
      "\n",
      "\n",
      "用户22是:白玉无尘...\n",
      "内容是：\n",
      "啊？你咋长胡子了？\n",
      "\n",
      "\n",
      "用户23是:蜀南熟男...\n",
      "内容是：\n",
      "广东人集体懵了，关于硬菜我有点不太懂了...\n",
      "\n",
      "\n",
      "用户24是:吃了两碗...\n",
      "内容是：\n",
      "手机要报废，每天不定时失去信号。上午媳妇给我打电话没打通，微信发语音也没接，中午回去就质问我怎么回事。我说手机没信号，还没有无线网。媳妇:你就不会开流量？我解释\n",
      "\n",
      "\n",
      "用户25是:砸妳家玻...\n",
      "内容是：\n",
      "5万一个月，每天工作15小时你愿意吗？网友：我拉着全家一起来。\n",
      "\n",
      "\n",
      "用户26是:薄荷记账...\n",
      "内容是：\n",
      "有个男人去停车场取车,发现自己的车头灯被人撞坏了。于是，他赶紧环视四周，但没有发现可疑的肇事车辆。正在他火冒三丈之时，突然雨刷下压着的一张纸条引起了他的注意。这\n",
      "\n",
      "\n",
      "用户27是:抚一缕漠...\n",
      "内容是：\n",
      "哈哈哈哈哈哈哈哈，给我笑抽了\n",
      "\n",
      "\n",
      "用户28是:傻妞也\n",
      "内容是：\n",
      "高手在民间，厉害了我的哥\n",
      "\n",
      "\n",
      "用户29是:米臭一白...\n",
      "内容是：\n",
      "那个村我也知道了。叫王富贵村\n",
      "\n",
      "\n",
      "用户210是:米拉扬\n",
      "内容是：\n",
      "小姐姐，别人都是人造革，你是真的皮啊\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_03.png\" align=\"absmiddle\">\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_03.png\" align=\"absmiddle\">\n",
      "\n",
      "\n",
      "用户211是:鱼歌浅唱...\n",
      "内容是：\n",
      "逛超市接到老妈电话，反复叮嘱我不要买菜，她刚刚去我家送过。回到家看到厨房好大一袋群英荟萃……“妈~您送的菜只是萝卜啊？”“对啊！白萝卜腌高丽咸菜、心里美檫丝儿凉\n",
      "\n",
      "\n",
      "用户212是:傻妞也\n",
      "内容是：\n",
      "爆笑失误合集，笑死人不偿命第一组合\n",
      "\n",
      "\n",
      "用户213是:慕随心グ...\n",
      "内容是：\n",
      "十三亿人同意了这门亲事，祝你们性福\n",
      "\n",
      "\n",
      "用户214是:今日好吧...\n",
      "内容是：\n",
      "糗友们，有缘再见吧\n",
      "\n",
      "\n",
      "用户215是:捻一丝江...\n",
      "内容是：\n",
      "麻将在世界上有多火\n",
      "\n",
      "\n",
      "用户216是:机智的G...\n",
      "内容是：\n",
      "真担心这样的熊孩子会不会被揍死[笑cry]​​​\n",
      "\n",
      "\n",
      "用户217是:叶氏柴窑...\n",
      "内容是：\n",
      "我们这算是输在了起跑线上了？近日，一份5岁小朋友的简历在社交平台广泛传播，孩子是”复（旦）二代”，2岁开始听并歌唱诗歌，5岁会唱百首古诗；英文年阅读量超过500\n",
      "\n",
      "\n",
      "用户218是:傻妞也\n",
      "内容是：\n",
      "虽然很精彩，结果不满意\n",
      "\n",
      "\n",
      "用户219是:当代艺术...\n",
      "内容是：\n",
      "哈哈哈哈哈\n",
      "\n",
      "\n",
      "用户220是:猫了个咪...\n",
      "内容是：\n",
      "你和我对视半天不说话只发信息，是想要闹哪样。。。\n",
      "\n",
      "\n",
      "20\n",
      "用户31是:黄佳美发...\n",
      "内容是：\n",
      "下次剪头还敢睡觉吗？\n",
      "\n",
      "\n",
      "用户32是:天真的恶...\n",
      "内容是：\n",
      "为什么我这么喜欢让子弹飞这部电影？\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_00.png\" align=\"absmiddle\">\n",
      "\n",
      "\n",
      "用户33是:水晶珠宝...\n",
      "内容是：\n",
      "机场着水杯有点不人道啊、根本放不下来！\n",
      "\n",
      "\n",
      "用户34是:baib...\n",
      "内容是：\n",
      "我前男友人长得丑又矮又没钱所以给自己起个名字叫恨钱不成山。后来，机缘巧合发财了人就飘了又换了一个名字叫捶死在座的各位。前几天碰见豆豆，聊到他了，据说现在在医院，\n",
      "\n",
      "\n",
      "用户35是:半根莲藕...\n",
      "内容是：\n",
      "你在老师办公室发现老师们哪些奇葩的事？\n",
      "\n",
      "\n",
      "用户36是:若风89...\n",
      "内容是：\n",
      "找家的味道:一大叔，在6楼，把自家的狗狗，放进电梯，按下负1楼，让它一个人坐下去。他老伴，在负1楼接着，又把狗狗单独放电梯，按6楼，让它回家。如此，练习N次，我\n",
      "\n",
      "\n",
      "用户37是:金小莲\n",
      "内容是：\n",
      "忍一时越想越气，退一步越想越亏😁\n",
      "\n",
      "\n",
      "用户38是:半根莲藕...\n",
      "内容是：\n",
      "如果梁山好汉也用微信......\n",
      "\n",
      "\n",
      "用户39是:金小莲\n",
      "内容是：\n",
      "猫:我说自己不是猴，有人信吗\n",
      "\n",
      "\n",
      "用户310是:傻妞也\n",
      "内容是：\n",
      "为什么都不去跳广场舞了\n",
      "\n",
      "\n",
      "用户311是:.微笑*...\n",
      "内容是：\n",
      "对门搬来一个好像做生意的，前些天出门碰到他，感觉很面熟，打了招呼，他迟疑后向我回应，过了几天，慢慢熟悉了今早在地下停车位，发现他的面包车后边被蹭了，而我的车前边\n",
      "\n",
      "\n",
      "用户312是:砸妳家玻...\n",
      "内容是：\n",
      "事实证明，挖掘机比女人更有魅力！\n",
      "\n",
      "\n",
      "用户313是:King...\n",
      "内容是：\n",
      "当喵喵发现自己被偷拍后……哈哈哈哈哈被这小表情简直了\n",
      "\n",
      "\n",
      "用户314是:该用户被...\n",
      "内容是：\n",
      "看到这张图，我总想起星爷在《国产凌凌漆》里拿着鞋说：你以为这是一双鞋，其实它是吹风筒图片转移知乎\n",
      "\n",
      "\n",
      "用户315是:半根莲藕...\n",
      "内容是：\n",
      "为什么现在剩女越来越多了？这就是男生难找老婆的原因\n",
      "\n",
      "\n",
      "用户316是:窝里斗窝...\n",
      "内容是：\n",
      "来自外国网友们的分享“用一张图来概括有孩子后的生活”。过分真实的今日份恐孩。。\n",
      "\n",
      "\n",
      "用户317是:吃个榴莲...\n",
      "内容是：\n",
      "lz闺蜜欢欢兴奋的跑过来“我就说男人都很绅士嘛，遇到事沉着冷静嘛，不像女人。。。”lz一脸懵逼，欢欢“就刚刚，我误入男厕所，他们只是哈哈大笑，没有大呼小叫哎”我\n",
      "\n",
      "\n",
      "用户318是:蜀南熟男...\n",
      "内容是：\n",
      "老板新请来的秘书，我注意她很久了\n",
      "\n",
      "\n",
      "用户319是:当代艺术...\n",
      "内容是：\n",
      "哈哈哈哈哈\n",
      "\n",
      "\n",
      "用户320是:雁ぺ南飞...\n",
      "内容是：\n",
      "网友发现了一只蚁后，于是带回了家放在试管里，亲眼见证了一个蚂蚁帝国的诞生​\n",
      "\n",
      "\n",
      "20\n",
      "用户41是:留恋那抹...\n",
      "内容是：\n",
      "触电也上瘾？！不行再来一次😂\n",
      "\n",
      "\n",
      "用户42是:人参和醋...\n",
      "内容是：\n",
      "你女朋友小时候的样子\n",
      "\n",
      "\n",
      "用户43是:蜀南熟男...\n",
      "内容是：\n",
      "翻马云微博笑死我了，果然是沙雕网友欢乐多\n",
      "\n",
      "\n",
      "用户44是:微醺♀\n",
      "内容是：\n",
      "女同事时不时地就会显摆一下她的好老公，说打了都不还手。下班回到家，我问老公:“老公，你说，如果我们吵架了，我打了你，你会打我吗？”老公斩钉截铁的说:“不可能的！\n",
      "\n",
      "\n",
      "用户45是:离雪沫阳...\n",
      "内容是：\n",
      "出太阳了，可以晒晒内内了\n",
      "\n",
      "\n",
      "用户46是:神评湿\n",
      "内容是：\n",
      "真是太气愤了，丈母娘为了不帮我俩带孩子，居然自己也生了个二胎……\n",
      "\n",
      "\n",
      "用户47是:日邢一珊...\n",
      "内容是：\n",
      "纯手动天窗~😂\n",
      "\n",
      "\n",
      "用户48是:又一盏素...\n",
      "内容是：\n",
      "怀念啊……\n",
      "\n",
      "\n",
      "用户49是:尛尛槑\n",
      "内容是：\n",
      "兄弟就是拿来坑的\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_57.png\" align=\"absmiddle\">\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_57.png\" align=\"absmiddle\">\n",
      "\n",
      "\n",
      "用户410是:窝里斗窝...\n",
      "内容是：\n",
      "【街头现神器！闯红灯就喷你[允悲]】前不久，湖北黄石大冶街头出现一种智能道闸，遇到行人闯红灯，它会语音提示，闪红灯和绿色光幕，并自动喷出水雾。这是交警的原创神器\n",
      "\n",
      "\n",
      "用户411是:我是煮茶...\n",
      "内容是：\n",
      "我打篮球左脚有时不得劲，我跟我妈说起这事。我妈说：“你小时候，你爸给你买了双雨靴，我说买小了，你爸那个犟脾气，硬说没买小，把你左脚拼命往里塞，塞是塞进去了，却不\n",
      "\n",
      "\n",
      "用户412是:或许不知...\n",
      "内容是：\n",
      "买了一包盐，两块钱。给了10块钱，找回一张8块钱。。。总觉得哪里不对，却又找不出哪里不对。。。\n",
      "\n",
      "\n",
      "用户413是:雁ぺ南飞...\n",
      "内容是：\n",
      "让我们看看一个机智幽默的崔永元！\n",
      "\n",
      "\n",
      "用户414是:蜀南熟男...\n",
      "内容是：\n",
      "爆笑车祸GIF图：确认过眼神，你是那个让我哉的人\n",
      "\n",
      "\n",
      "用户415是:为你变了...\n",
      "内容是：\n",
      "一款比法拉利还快的自行车，法拉利简直弱爆了！\n",
      "\n",
      "\n",
      "用户416是:南宁土著...\n",
      "内容是：\n",
      "狗子:太重了，我不拉了，走你……\n",
      "\n",
      "\n",
      "用户417是:鱼歌浅唱...\n",
      "内容是：\n",
      "去二姨家做客，姨夫显宝般推出一台崭新的空气净化器……“看看！看看！这是我参加**公司活动抽奖抽到的！一等奖哪！那些老头老太太们都老眼馋了！”“您这是又买了他们公\n",
      "\n",
      "\n",
      "用户418是:Regr...\n",
      "内容是：\n",
      "话不多说，这个作者有多优秀是你能知道的吗？\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_03.png\" align=\"absmiddle\">\u0001<img src=\"https://static.qiushibaike.com/static/images/emoji/qb_s_03.png\" align=\"absmiddle\">\n",
      "\n",
      "\n",
      "用户419是:阿呆家的...\n",
      "内容是：\n",
      "每到吃螃蟹的时候,总有些网友格外优秀!\n",
      "\n",
      "\n",
      "用户420是:wz02...\n",
      "内容是：\n",
      "跑就跑掉了\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import urllib.request\n",
    "def getcontent(url,page):\n",
    "    #模拟成浏览器\n",
    "    headers = (\"User-Agent\",\n",
    "               \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\")\n",
    "    opener= urllib.request.build_opener()\n",
    "    opener.addheaders=[headers]\n",
    "    #将opener安装为全局\n",
    "    urllib.request.install_opener(opener)\n",
    "    data=urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "    #print(data)\n",
    "    #构建对应用户提取的正则表达式\n",
    "    userpat='<span class=\"recmd-name\">(.*?)</span>'\n",
    "    #构建段子内容提取的正则表达式\n",
    "    contentpat='<a class=\"recmd-content\" href=\"/article/.*?\" target=\"_blank\" onclick=\"_hmt.push(.*?)\">(.*?)</a>'\n",
    "    #寻找出所有的用户\n",
    "    userlist=re.compile(userpat,re.S).findall(data)\n",
    "    #寻找出所有的内容\n",
    "    contentlist=re.compile(contentpat,re.S).findall(data)\n",
    "    x=1\n",
    "    print(len(contentlist))\n",
    "#     for content in contentlist:\n",
    "#         print(content,end=' ')\n",
    "    #通过for循环遍历段子内容并将内容分别赋给对应的变量\n",
    "    for content in contentlist:\n",
    "        content = content[1]\n",
    "        #content=content.replace(\"\\n\",\"\")\n",
    "        #用字符串作为变量名，先将对应字符串先赋给一个变量\n",
    "        name=\"content\"+str(x)\n",
    "        #通过exec()函数实现用字符串作为变量名并赋值\n",
    "        exec(name+'=content')\n",
    "        x+=1\n",
    "    y=1\n",
    "    #通过for循环遍历用户，并输出该用户对应的内容\n",
    "    for user in userlist:\n",
    "        name=\"content\"+str(y)\n",
    "        print(\"用户\"+str(page)+str(y)+\"是:\"+user)\n",
    "        print(\"内容是：\")\n",
    "        exec(\"print(\"+name+\")\")#????\n",
    "        print(\"\\n\")\n",
    "        y+=1\n",
    "#别获取各页的段子，通过for循环可以获取多页\n",
    "for i in range(1,5):\n",
    "    url=\"http://www.qiushibaike.com/8hr/page/\"+str(i)\n",
    "    getcontent(url,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题-exec()函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微信爬虫实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " 微信爬虫稍难一些。因为在爬取的时候，经常会被官方封杀IP。  \n",
    " 先看下人工方法如何实现网页爬取，再编写爬虫实现自动化。  \n",
    " 入口：http://weixin.sogou.com/  \n",
    " 确定关键词：type\\query\\page  \n",
    " 大体思路：第一步，检索对应关键词得到相应的文章检索结果，并在该页面中将文章的链接提取出来；  \n",
    " 第二步，将文章链接提取出来之后，根据这些链接地址采集文章的具体标题和内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "正则表达式：'<div class=\"txt-box\">.*?(http://.*?)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
