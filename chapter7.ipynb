{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学会使用Fiddler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫浏览器的伪装技术"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import http.cookiejar\n",
    "url = \"https://news.163.com/18/1221/22/E3J6A92F0001875P.html\"\n",
    "cjar = http.cookiejar.CookieJar()\n",
    "proxy = urllib.request.ProxyHandler({'http':\"127.0.0.1:8888\"})\n",
    "opener = urllib.request.build_opener(proxy,urllib.request.HTTPHandler,urllib.request.HTTPCookieProcessor(cjar))\n",
    "urllib.request.install_opener(opener)\n",
    "data = urllib.request.urlopen(url).read()\n",
    "fhandle = open(\"d:/python/8_1.html\",\"wb\")\n",
    "fhandle.write(data)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**如果使用fiddler作为代理服务器，所爬取的网址要以具体文件或者“/”结尾，如果有具体文件，直接写改文件的具体网址即可，“https://news.163.com/18/1221/22/E3J6A92F0001875P.html”这种写法是合法的。  \n",
    "如果被爬取的是一个文件夹，比如”http://www.baidu.com\"，此时爬取的是一个目录（文件夹），所以要以“/”结尾，“http://www.baidu.com\"这种是合法的，而”http://www.baidu.com\"这种则可能会出现错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "referer字段的值一般设为要爬取的网页的域名地址或对应的网站的主页网址。  \n",
    "Accept-Encoding设置为gzip,deflate,可能会出现乱码，此时只需要将该字段信息省略不写或者将该字段信息设置为utf-8或gb2312即可。因为如果设置为gzip,deflate，从服务器返回的这两种压缩的代码，没有浏览器解码，会出现乱码问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）常见反爬虫机制：通过分析用户请求的Headers信息进行反爬虫，通过检测用户行为进行反爬虫，比如通过判断同一个IP在短时间内是否频繁访问对应网站等进行分析；通过动态页面增加爬虫爬取的难度，达到反爬虫的目的。  \n",
    "2）使用fiddler作为代理服务器，所爬取的网址要以具体文件或者“/”结尾。  \n",
    "3）referer字段的值一般可以设置为要爬取的网页的域名地址或对应网站的主页网址。  \n",
    "4）在实际项目中，要伪装成浏览器，我们不一定要将fiddler设置为代理服务器，上面实例1、2设置Fiddler为代理服务器主要为了方便抓包，从而方便调试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫的定向爬取技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是定向爬取技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定向爬取的相关步骤与策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定向爬取技术主要需要解决3个问题：  \n",
    "1）清晰地定义好爬虫的爬取目标，规划好主题。  \n",
    "2）建立好爬取网址的过滤筛选规则以及内容的过滤筛选规则。  \n",
    "3）建立好URL排序算法，让爬虫能够明确优先爬取哪些网页、以什么顺序爬取待爬取的网页。不一样的顺序去爬取，可能会导致不一样的爬取效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定向爬虫实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "观察点击“加载更多评论”前后的请求网址变化，总结视频评论的URL地址格式为：\"https://video.coral.qq.com/article/视频编号/\"\n",
    "\"https://video.coral.qq.com/varticle/1005669305/comment/v2?callback=_varticle1005669305commentv2&orinum=10&oriorder=o&pageflag=1&cursor=0&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=9&_=1545445817681\"(起始页)\n",
    "\"https://video.coral.qq.com/varticle/1005669305/comment/v2?callback=_varticle1005669305commentv2&orinum=10&oriorder=o&pageflag=1&cursor=6399626802072370905&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=9&_=1545445817684\"(加载一次)\n",
    "\"https://video.coral.qq.com/varticle/1005669305/comment/v2?callback=_varticle1005669305commentv2&orinum=10&oriorder=o&pageflag=1&cursor=6464381694372580731&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=9&_=1545445817685\"(加载两次)  \n",
    "点击网址查看，数据信息，发现评论id，被评论id，评论内容格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import http.cookiejar\n",
    "import re\n",
    "# 设置视频编号\n",
    "# 大话西游之大圣娶亲https://v.qq.com/x/cover/fse52rd4klx7qn2.html?\n",
    "vid = \"1005669305\"\n",
    "# 设置评论起始标编号\n",
    "cursorid = \"0\"\n",
    "# 构造真实评论请求网址\n",
    "url = \"https://video.coral.qq.com/varticle/\"+vid+\"/comment/v2?callback=_varticle1005669305commentv2&orinum=10&oriorder=o&pageflag=1&cursor=\"+cursorid+\"&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=9&_=1545445817681\"\n",
    "# 设置头信息伪装成浏览器\n",
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gb2312, utf-8\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\",\n",
    "    \"Host\": \"video.coral.qq.com\",\n",
    "}\n",
    "#设置cookie\n",
    "cjar = http.cookiejar.CookieJar()\n",
    "opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cjar))\n",
    "headall=[]\n",
    "for key,value in headers.items():\n",
    "    item = (key,value)\n",
    "    headall.append(item)\n",
    "opener.addheaders = headall\n",
    "urllib.request.install_opener(opener)\n",
    "#爬取该网页\n",
    "data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "#分别构建筛选id、用户名、评论内容等信息的正则表达式\n",
    "idpat = '\"targetid\":\"(.*?)\"'#书上是“id”：\n",
    "userpat = '\"userid\":\"(.*?)\",'#书上是“nick\"\n",
    "conpat = '\"content\":\"(.*?)\",'\n",
    "#分别根据正则表达式查找所有id、用户名、评论内容等信息\n",
    "idlist = re.compile(idpat,re.S).findall(data)\n",
    "print('1')\n",
    "userlist = re.compile(userpat,re.S).findall(data)\n",
    "print(eval('u\"'+userlist[1]+'\"'))\n",
    "conlist = re.compile(conpat,re.S).findall(data)\n",
    "print(len(userlist))\n",
    "#通过循环将获取到的各信息遍历出来\n",
    "for i in range(0,10):\n",
    "    #输出对应信息，并对字符串进行unicode编码，正常显示\n",
    "    try:\n",
    "        print(\"用户名是：\"+eval('u\"'+userlist[i]+'\"'))\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    print(\"评论内容是：\"+eval('u\"'+conlist[i]+'\"'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是上方只能获取一页的评论，修改之后，获取多页："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "第1页评论内容\n",
      "用户名是：234781542\n",
      "评论内容是：蓝洁瑛，一路走好。愿天堂没有痛苦。\n",
      "\n",
      "\n",
      "用户名是：102140877\n",
      "评论内容是：大话西游要重映，刚刚重温一遍。。。割了吧。。。十多年前看不懂，是笑着看完，十多年后看懂了，哭着看完。有多少男孩像条狗一样，没有珍惜曾经的那份感情，现如今却已成为难以抹灭的伤痕。总会有一个女孩出现，让男孩变成男人，再见青春，再见我的爱人，如果上天再给我一次机会，我会紧抱着你对你说，我爱你，永远都不分开\n",
      "\n",
      "\n",
      "\n",
      "用户名是：12544640\n",
      "评论内容是：别人笑我太疯癫，我笑他人看不穿。星爷电影表达的意思不到一定年纪是不会懂的，很多台词都很多深意，(那个人好像一条狗)，这句话十几年后我才明白。表达的意思太超前，当年大部分人都看不懂，大话西游这个经典无法超越，连很多台词至今都无法难忘。星爷是我最佩服的一个电影人。看了他很多电影，却没买过一张电影票，我欠星爷一张票。\n",
      "\n",
      "\n",
      "用户名是：179528875\n",
      "评论内容是：说真的这么多年了，第一次认真看完这部电影。\n",
      "\n",
      "\n",
      "用户名是：117647702\n",
      "评论内容是：紫霞到底爱的是那个放荡不羁的至尊宝，还是脚踏七彩祥云，身披金甲圣衣，法力无边的齐天大圣？\n",
      "\n",
      "\n",
      "用户名是：236684428\n",
      "评论内容是：看完前任三又来看这个的举手\n",
      "\n",
      "\n",
      "用户名是：32502918\n",
      "评论内容是：如今我们再次观看大话西游，我们谁都想找一个紫霞仙子，也想做一个至尊宝，但是最后到头来我们都是那城墙下面的看客，一个个回忆着自己青涩懵懂的青春年华，不经意间竟已到了迟暮之年\n",
      "\n",
      "\n",
      "用户名是：407405399\n",
      "评论内容是：我也是因为惦念蓝洁瑛才认真看完此电影，人漂亮，演技也好，就是命不好\n",
      "\n",
      "\n",
      "用户名是：76763831\n",
      "评论内容是：这演得怪怪的，看不懂！演的是啥呀\n",
      "\n",
      "\n",
      "用户名是：20128166\n",
      "评论内容是：感情真的好细腻，可惜了朱茵和星星的爱情\n",
      "\n",
      "\n",
      "---------------------------\n",
      "第2页评论内容\n",
      "用户名是：25586719\n",
      "评论内容是：每一遍看都会哭一遍 真的好感人\n",
      "\n",
      "\n",
      "用户名是：183227818\n",
      "评论内容是：星爷用一生回应了那一句“他很花心”，如今满头花白孑然一人。用他的智慧与青春年华，给世人留下了无数欢声笑语和人生感悟！“从前直到现在，爱还在”《西游·降魔篇》中加的一句歌词道尽了心声，他活的真的好像一条狗。希望星爷在晚年找到自己的幸福~\n",
      "\n",
      "\n",
      "用户名是：26079512\n",
      "评论内容是：蜘蛛精走了，艳绝五台山终成绝响！愿天堂里没有痛苦！只有美好！\n",
      "\n",
      "\n",
      "用户名是：182929477\n",
      "评论内容是：白晶晶爱上的是孙悟空，而爱白晶晶的却是至尊宝，紫霞爱上的是至尊宝，可是最后爱上紫霞的却是孙悟空，而孙悟空与至尊宝又相差500年。当我还是至尊宝的时候，我能爱你但是我保护不了你，当我是孙悟空时，我能保护你，但是我不能爱你。\n",
      "\n",
      "\n",
      "用户名是：258435972\n",
      "评论内容是：每个人都相当至尊宝，有梦想的日子，而当他带上紧箍咒的时候，就会失去很多，生活就像紧箍咒，社会就是牛魔王，父母就像唐僧，紫霞就是梦想，而他活的像条狗。\n",
      "\n",
      "\n",
      "用户名是：84101074\n",
      "评论内容是：有多少人是看完前任3再回来看大话西游的，报个到\n",
      "\n",
      "\n",
      "用户名是：621357008\n",
      "评论内容是：西游记有九九八十ー难，八十难都留给了猴子，但唯独女儿国这一难给了唐僧。小时候看西游记，觉得女儿国好无聊，这一劫过的好简单，什么妖怪都没有就这么过去了，长大后オ知道这一劫过得撕心裂肺。小时候以为唐僧躲过了一劫，长大了オ知道原来女儿国是最难渡的情劫，生劫易渡，情劫难了，唐僧错过了一生。唐僧这一生只撒过两次谎，一次骗孙悟空戴上金箍，一次骗女儿国国王还会回来。一次骗到了齐天大圣美猴王ー个徒弟，一次骗到了女儿国国王娇柔痴情的一颗心。国王在等御弟哥哥（唐僧）下辈子红妆相娶，却不知西天成佛再无来生。唐僧这一生终究还是没做到不负如来不负卿．若有来生再相遇，愿弃了一袭袈裟，舍了ー身佛法，消了那满头戒疤，倾我十世修行，只为换取与你海角天涯，小舍篱笆，青丝白发！可哪有什么来生？从来都只有今生，此时，此刻。人生中无数次错过就是错过，即使高贵如国王，开挂如圣僧，也抵不过一句有缘无份。世间能放下的，都是你未曾抬起的。爱个人与爱众生，是没有区别。若有来生，我舍我的王权富贵，你舍你的戒律清规．可惜没有来生了。我做了一个梦，我梦见你蓄起了长发，我们一起慢慢变老，可是，你不快乐。国王满目流转地都是深情，却只牵衣袖不牵手。喜欢就会放肆，但爱是克制。最后她为爱而退让，为爱而成全，亲手为所爱之人披上袈！\n",
      "\n",
      "\n",
      "用户名是：88271367\n",
      "评论内容是：你知不知道什么是铛铛铛？\n",
      "\n",
      "\n",
      "用户名是：535601656\n",
      "评论内容是：小编：请问星爷您对待黑粉是怎么看的。 星爷：\n",
      "\n",
      "\n",
      "用户名是：768775114\n",
      "评论内容是：女神走好 这个月是怎么了 难道是一个时代的落幕吗\n",
      "\n",
      "\n",
      "---------------------------\n",
      "第3页评论内容\n",
      "用户名是：251794824\n",
      "评论内容是：每个人的心里都住着一个人，\n",
      "无论过去，还是现在\n",
      "过去的，不是放不下，而是无法忘怀\n",
      "现在的，不是不珍惜，而是无能为力\n",
      "未来的，不是不期盼，而是无法预知\n",
      "我不敢说我还爱着\n",
      "可心还是会被触碰\n",
      "我试着去遗忘曾经\n",
      "可却不知何时是头\n",
      "我不会说我还在等待\n",
      "等待一份不分开的爱\n",
      "等待一个不离开的人\n",
      "\n",
      "\n",
      "用户名是：36994533\n",
      "评论内容是：基本上过个三五年就要再看一遍星爷的这片。每次感受都会不一样，人的心境 面对生活后的感悟 全部不同！\n",
      "\n",
      "\n",
      "用户名是：176486376\n",
      "评论内容是：大话西游要重映，刚刚重温一遍。。。割了吧。。。十多年前看不懂，是笑着看完，十多年后看懂了，哭着看完。有多少男孩像条狗一样，没有珍惜曾经的那份感情，现如今却已成为难以抹灭的伤痕。总会有一个女孩出现，让男孩变成男人，再见青春，再见我的爱人，如果上天再给我一次机会，我会紧抱着你对你说，我爱你，永远都不分开\n",
      "\n",
      "\n",
      "用户名是：223446959\n",
      "评论内容是：后来我们不再联系，我还是会想起你，你还好吗？\n",
      "\n",
      "\n",
      "用户名是：874714623\n",
      "评论内容是：哈哈哈。。说实话，小时候第一次看的时候，最后结尾武士说“那人长的好像狗的时候”我当时觉得特别的无厘头，一直就纳闷，感觉怎么看都不像，不知道他这句话的梗，或者说话的角度从哪里来的，就是不懂，但是绝对没有想去笑，～～～～～看网友的解析回想自己的经历，结合电影里的这句，感觉自己活的就像条狗，确实，真的说到我心坎里去了，至少大学之前我觉得自己是至尊宝，进入社会参加工作后，我越来越觉得自己活的像条狗，好了，不说了，好电影，好电影，\n",
      "\n",
      "\n",
      "用户名是：49524710\n",
      "评论内容是：10岁的我是笑着看完的。22岁的我是哭着看完了\n",
      "\n",
      "\n",
      "用户名是：664430185\n",
      "评论内容是：一生所爱.朱茵\n",
      "\n",
      "\n",
      "用户名是：73375052\n",
      "评论内容是：一路走好，蓝洁瑛。永远的蜘蛛精\n",
      "\n",
      "\n",
      "用户名是：64066550\n",
      "评论内容是：经典，小时候看不懂，看懂已经是剧中人！\n",
      "\n",
      "\n",
      "用户名是：725719790\n",
      "评论内容是：我知道有一天\n",
      "\n",
      "\n",
      "用户名是：251794824\n",
      "评论内容是：每个人的心里都住着一个人，\n",
      "无论过去，还是现在\n",
      "过去的，不是放不下，而是无法忘怀\n",
      "现在的，不是不珍惜，而是无能为力\n",
      "未来的，不是不期盼，而是无法预知\n",
      "我不敢说我还爱着\n",
      "可心还是会被触碰\n",
      "我试着去遗忘曾经\n",
      "可却不知何时是头\n",
      "我不会说我还在等待\n",
      "等待一份不分开的爱\n",
      "等待一个不离开的人\n",
      "\n",
      "\n",
      "用户名是：36994533\n",
      "评论内容是：基本上过个三五年就要再看一遍星爷的这片。每次感受都会不一样，人的心境 面对生活后的感悟 全部不同！\n",
      "\n",
      "\n",
      "用户名是：176486376\n",
      "评论内容是：大话西游要重映，刚刚重温一遍。。。割了吧。。。十多年前看不懂，是笑着看完，十多年后看懂了，哭着看完。有多少男孩像条狗一样，没有珍惜曾经的那份感情，现如今却已成为难以抹灭的伤痕。总会有一个女孩出现，让男孩变成男人，再见青春，再见我的爱人，如果上天再给我一次机会，我会紧抱着你对你说，我爱你，永远都不分开\n",
      "\n",
      "\n",
      "用户名是：223446959\n",
      "评论内容是：后来我们不再联系，我还是会想起你，你还好吗？\n",
      "\n",
      "\n",
      "用户名是：874714623\n",
      "评论内容是：哈哈哈。。说实话，小时候第一次看的时候，最后结尾武士说“那人长的好像狗的时候”我当时觉得特别的无厘头，一直就纳闷，感觉怎么看都不像，不知道他这句话的梗，或者说话的角度从哪里来的，就是不懂，但是绝对没有想去笑，～～～～～看网友的解析回想自己的经历，结合电影里的这句，感觉自己活的就像条狗，确实，真的说到我心坎里去了，至少大学之前我觉得自己是至尊宝，进入社会参加工作后，我越来越觉得自己活的像条狗，好了，不说了，好电影，好电影，\n",
      "\n",
      "\n",
      "用户名是：49524710\n",
      "评论内容是：10岁的我是笑着看完的。22岁的我是哭着看完了\n",
      "\n",
      "\n",
      "用户名是：664430185\n",
      "评论内容是：一生所爱.朱茵\n",
      "\n",
      "\n",
      "用户名是：73375052\n",
      "评论内容是：一路走好，蓝洁瑛。永远的蜘蛛精\n",
      "\n",
      "\n",
      "用户名是：64066550\n",
      "评论内容是：经典，小时候看不懂，看懂已经是剧中人！\n",
      "\n",
      "\n",
      "用户名是：725719790\n",
      "评论内容是：我知道有一天\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import http.cookiejar\n",
    "import re\n",
    "# 设置视频编号\n",
    "# 大话西游之大圣娶亲\n",
    "vid = \"1005669305\"\n",
    "# 刚开始评论ID\n",
    "cursorid = \"0\"\n",
    "# 构造真实评论请求网址\n",
    "url = \"https://video.coral.qq.com/varticle/\"+vid+\"/comment/v2?callback=_varticle1005669305commentv2&orinum=10&oriorder=o&pageflag=1&cursor=\"+cursorid+\"&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=9&_=1545445817681\"\n",
    "# 设置头信息伪装成浏览器\n",
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gb2312, utf-8\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\",\n",
    "    \"Host\": \"video.coral.qq.com\",\n",
    "}\n",
    "#设置cookie\n",
    "cjar = http.cookiejar.CookieJar()\n",
    "opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cjar))\n",
    "headall=[]\n",
    "for key,value in headers.items():\n",
    "    item = (key,value)\n",
    "    headall.append(item)\n",
    "opener.addheaders = headall\n",
    "urllib.request.install_opener(opener)\n",
    "#建立一个自定义函数craw(vid,comid),实现自动爬取对应评论网页并返回爬取数据\n",
    "def craw(vid,comid):\n",
    "    url = \"https://video.coral.qq.com/varticle/\"+vid+\"/comment/v2?callback=_varticle1005669305commentv2&orinum=10&oriorder=o&pageflag=1&cursor=\"+cursorid+\"&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=9&_=1545445817681\"\n",
    "    data = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "    return data\n",
    "#分别构建筛选id、用户名、评论内容等信息的正则表达式\n",
    "idpat = '\"targetid\":\"(.*?)\"' #python中单引号和双引号\n",
    "userpat = '\"userid\":\"(.*?)\",'\n",
    "conpat = '\"content\":\"(.*?)\",'\n",
    "cursoridpat = '\"last\":\"(.*?)\",'\n",
    "#第一层循环，代表爬取多少页，每一次外层循环爬取一页\n",
    "for i in range(1,4):\n",
    "    print(\"---------------------------\")\n",
    "    print(\"第\"+str(i)+\"页评论内容\")\n",
    "    data = craw(vid,cursorid)\n",
    "    idlist = re.compile(idpat,re.S).findall(data)\n",
    "    userlist = re.compile(userpat,re.S).findall(data)\n",
    "    conlist = re.compile(conpat,re.S).findall(data)\n",
    "    cursorid = re.compile(cursoridpat,re.S).findall(data)[0]\n",
    "    #第二层循环，根据爬取的结果提取并处理每条评论的信息，一页10条评论\n",
    "    for j in range(0,10):\n",
    "        try:\n",
    "            print(\"用户名是：\"+eval('u\"'+userlist[j]+'\"'))\n",
    "            print(\"评论内容是：\"+eval('u\"'+conlist[j]+'\"'))\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    #将comid改变为该页的最后一条评论id，实现不断自动加载\n",
    "    #print(\"！！！！长度为\"+str(len(idlist))\n",
    "    #cursorid=idlist[9]\n",
    "\n",
    "#分别根据正则表达式查找所有id、用户名、评论内容等信息\n",
    "idlist = re.compile(idpat,re.S).findall(data)\n",
    "userlist = re.compile(userpat,re.S).findall(data)\n",
    "conlist = re.compile(conpat,re.S).findall(data)\n",
    "#通过循环将获取到的各信息遍历出来\n",
    "for i in range(0,10):\n",
    "    #输出对应信息，并对字符串进行unicode编码，正常显示\n",
    "    print(\"用户名是：\"+eval('u\"'+userlist[i]+'\"'))\n",
    "    print(\"评论内容是：\"+eval('u\"'+conlist[i]+'\"'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1）爬虫的定向爬取技术就是根据设置的主体，对要爬取的网址或者网页中的内容进行筛选。  \n",
    "2）互联网信息海量，为高效，应根据设定的主体，拟定出对应的爬取策略与爬取规则。  \n",
    "3）python中，进行信息筛选的方法策略主要有：正则表达式、Xpath表达式、通过xslt筛选。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第10章 了解爬虫框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 什么是python爬虫框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬虫项目的半成品，将常见爬虫功能的实现代码写好（将一些常见的功能代码、业务逻辑等进行封装，从而能够让我们更高效），留下一些接口，后续根据实际情况，编写少量需要变动的代码部分，按照需求调用接口，即可实现一个爬虫项目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的爬虫框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大部分框架大同小异，只需深入了解一种接口。我们以python爬虫框架中的Scrapy框架作为重点讲解内容。  \n",
    "python常见框架：  \n",
    "1）Scrapy框架  \n",
    "2）Crawley框架  \n",
    "3）Portia框架  \n",
    "4）newspaper框架  \n",
    "5）python-goose框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 认识Scrapy框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用领域：网络爬虫开发、数据挖掘、数据监测、自动化测试等。  \n",
    "官网：http://scrapy.org/  \n",
    "特征：开源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 认识Crawley框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官网：http://project.crawley-cloud.com/  \n",
    "可在官网对其了解并下载框架。  \n",
    "主要特点：  \n",
    "1）高速爬取对应网站内容  \n",
    "2）可以将爬取到的内容轻松地存储到关系型数据库中，比如MySQL、Oracle、SQLite、Postgres等。  \n",
    "3）可以将爬取到的数据导出为JSON、XML等格式。  \n",
    "4）支持非关系型数据库，比如MongoDB、CouchDB等。  \n",
    "5）支持使用命令行工具。  \n",
    "6）可以使用你喜欢的工具提取数据，比如使用XPath或者PyQuery等工具。  \n",
    "7）支持使用Cookie登录并访问那些只有登录才能够访问的网页。  \n",
    "8）简单易学。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 认识Portia框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征：允许没有任何编程基础的用户可视化地爬取网页的爬虫框架。  \n",
    "获取code：http://github.com/scrapinghub/portia/  \n",
    "网页版地址：https://portia.scrapinghub.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 认识newspaper框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用来提取新闻、文章以及内容分析的爬虫框架，是python的一个库。  \n",
    "GitHub上的主页地址：https://github.com/codelucas/newspaper  \n",
    "特点:  \n",
    "1)比较简洁  \n",
    "2)速度快  \n",
    "3)支持多线程  \n",
    "4)支持十多种语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 认识python-goose框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goose本来是一款Java写的文章提取工具，有人用python重写了Goose，并将重写后的Goose命名为Python-goose。  \n",
    "功能：文章提取  \n",
    "GitHub主页地址：https://github.com/grangier/python-goose  \n",
    "提取如下信息：  \n",
    "1）文章主体内容  \n",
    "2）元描述  \n",
    "3）元标签  \n",
    "4）文章中的任何YouTube/vIMEO视频  \n",
    "5）文章中的主要图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第11章 爬虫利器——Scrapy安装与配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows安装和配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置fiddler，取消软件对客户端连接的干扰。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可能需要Visual Studio的支持，建议安装专业版，社区版可能会出现很多问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64位版本对应问题：  \n",
    "import pip._internal  \n",
    "print(pip._internal.pep425tags.get_supported())  \n",
    "32位版本对应问题：  \n",
    "import pip;print(pip.pep425tags.get_supported())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAC安装和配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linux安装和配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第12章 开启Scrapy爬虫项目之旅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 认识Scrapy项目的目录结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用Scrapy进行爬虫项目管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cmd命令中输入：  \n",
    "scrapy startproject myfirstpjt\n",
    "**调出startproject的帮助信息：scrapy startproject -h**  \n",
    "输出：  \n",
    "**--logfile=FILE          log file. if omitted stderr will be used**  \n",
    "（指定日志文件file为指定日志文件的路径地址。  命令例子：scrapy startproject --logfile=\"../logf.log\" mypjt1  \n",
    "**--loglevel=LEVEL, -L LEVEL**  \n",
    "                        log level (default: DEBUG)  \n",
    "--loglevel=LEVEL,-L LEVEL参数主要控制日志信息的等级，默认为DEBUG模式，即会将对应调试信息都输出，其它值还有： \n",
    "CRITICAL 发生最严重的的错误  \n",
    "ERROR  发生了必须立即处理的错误  \n",
    "WARNING 出现一些警告信息，即存在潜在错误  \n",
    "INFO  输出一些提示信息  \n",
    "DEBUG 输出一些调试信息，常用于开发阶段  \n",
    "如果将日志等级设置为DEBUG最低等级，所有调试信息都会输出出来，如果只想输出一些警告以上的日志信息，可将日志等级设置为WARNING.  \n",
    "设置例子：scrapy startproject --loglevel=DEBUG mypjt2  \n",
    "--nolog                 disable logging completely  \n",
    "控制不输出日志信息，如:scrapy startproject --nolog mypjt3  \n",
    "**--profile=FILE          write python cProfile stats to FILE**  \n",
    "**--pidfile=FILE          write process ID to FILE**  \n",
    "**--set=NAME=VALUE, -s NAME=VALUE**  \n",
    "                        set/override setting (may be repeated)  \n",
    "--pdb                   enable pdb on failure  \n",
    "比如，我们希望日志文件存储在当前目录的上一层目录下，并且日志文件名为logf.txt，此时我们可以这样实现：  \n",
    "scrapy startproject --logfile=\"../logf.log\" mypjt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 常用工具命令"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种工具命令：一种为全局命令，一种为项目命令。  \n",
    "全局命令不需要依靠scrapy项目就可以在全局中直接运行，而项目命令必须要在scrapy项目中才能运行。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全局命令"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在项目目录之外，运行 scrapy -h，出现所有的全局命令。  \n",
    "Available commands:\n",
    "  bench         Run quick benchmark test（项目命令）  \n",
    "  check         Check spider contracts  \n",
    "  crawl         Run a spider  \n",
    "  edit          Edit spider  \n",
    "  fetch         Fetch a URL using the Scrapy downloader  \n",
    "  genspider     Generate new spider using pre-defined templates  \n",
    "  list          List available spiders  \n",
    "  parse         Parse URL (using its spider) and print the results  \n",
    "  runspider     Run a self-contained spider (without creating a project)  \n",
    "  settings      Get settings values  \n",
    "  shell         Interactive scraping console  \n",
    "  startproject  Create new project  \n",
    "  version       Print Scrapy version  \n",
    "  view          Open URL in browser, as seen by Scrapy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（1）fetch命令**  \n",
    "在cmd中使用“scrapy fetch 网址”显示爬取对应网址的过程。  \n",
    "此时，如果在项目目录之外使用该命令，则会调用scrapy默认的爬虫来进行网页爬取，如果在某个项目目录内使用该命令，则会调用该项目中的爬虫来进行网页的爬取。  \n",
    "在使用fetch命令的时候，同样可以使用某些参数进行相应的控制。  \n",
    "通过scrapy fetch -h列出所有可以使用的fetch相关参数。  \n",
    "比如，可以通过--headers参数来控制显示对应的爬虫网页头信息，可通过--nlog参数来控制不显示日志信息，同时，还可以通过--spider=SPIDER参数来控制使用哪个爬虫，通过--logfile=FILE参数来指定存储日志信息的文件，通过--loglevel=LEVEL参数来控制日志等级。  \n",
    "如：scrapy fetch --headers --nolog http://news.sina.com.cn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**runspider命令**  \n",
    "通过runspider命令可以实现不依托scrapy的爬虫项目，直接运行一个爬虫文件。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#保存first.py文件\n",
    "from scrapy.spiders import Spider\n",
    "\n",
    "class FirstSpider(Spider):\n",
    "    name = \"first\"\n",
    "    allowed_domains = [\"baidu.com\"]\n",
    "    start_urls = [\n",
    "        \"http://www.baidu.com\"\n",
    "    ]\n",
    "    \n",
    "    def parse(self,response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#输入命令\n",
    "scrapy runspider --loglevel=INFO first.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（3）settings命令**  \n",
    "如果在scrapy项目目录内使用settings命令，查看的是对应项目的配置信息，如果在scrapy项目目录外使用settings命令，查看的是scrapy默认配置信息。  \n",
    "myfirstpjt\\myfirstpjt文件夹中的settings.py文件，就是该项目的配置信息。  \n",
    "还可通过命令行查看该项目的配置信息，使用scrapy settings --get BOT_NAME查看配置信息中BOT_NAME对应的值。  \n",
    "可退出项目目录查看默scrapy认配置信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)shell命令**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过shell命令可以启动scrapy的交互终端（scrapy shell）。  \n",
    "scrapy的交互终端经常在开发及调试的时候用到，使用scrapy的交互终端可以实现在不启动scrapy爬虫的情况下，对网站相应进行调试，在交互终端中，也可以写一些python代码进行相应测试。  \n",
    "比如，使用shell命令为爬取百度首页创建一个交互终端环境，并设置为不输出日志信息：  \n",
    "**scrapy shell http://www.baidu.com --nolog**  \n",
    "执行该命令之后，会出现可以使用的scrapy对象及快捷命令，比如item、response、settings、spider等。进行交互模式后，在“>>>”后可以输入交互命令及相应的代码。  \n",
    "通过XPath表达式进行提取网页标题。“/html/head/title\"的意思是：提取该网页\\<html>标签下的\\<head>标签中的\\<title>标签对应的信息。我们知道，此时该标签中的信息即为该网页的标题信息。  \n",
    "**>>> ti=sel.xpath(\"/html/head/title\")**\n",
    "**>>> print(ti)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)startproject命令**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（6）version命令**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "通过version，可以直接显示scrapy的版本相关信息。  \n",
    "代码实现： scrapy version  \n",
    "输出：1.5.1  \n",
    "查看其它版本信息：scrapy version -v  \n",
    "输出：  \n",
    "Scrapy       : 1.5.1  \n",
    "lxml         : 4.1.0.0  \n",
    "libxml2      : 2.9.5\n",
    "cssselect    : 1.0.3\n",
    "parsel       : 1.5.1\n",
    "w3lib        : 1.19.0\n",
    "Twisted      : 18.9.0\n",
    "Python       : 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\n",
    "pyOpenSSL    : 17.2.0 (OpenSSL 1.0.2n  7 Dec 2017)\n",
    "cryptography : 2.0.3\n",
    "Platform     : Windows-10-10.0.14393-SP0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（7）view命令**  \n",
    "通过view命令，我们可以实现下载某个网页并用浏览器查看的功能。  \n",
    "比如，网易：scrapy view http://news.163.com/  \n",
    "执行该命令之后，会自动在浏览器中打开下载的该网页，地址为本地？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目命令"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进入项目，scrapy -h  \n",
    "Available commands:\n",
    "  bench         Run quick benchmark test\n",
    "  check         Check spider contracts\n",
    "  crawl         Run a spider\n",
    "  edit          Edit spider\n",
    "  fetch         Fetch a URL using the Scrapy downloader\n",
    "  genspider     Generate new spider using pre-defined templates\n",
    "  list          List available spiders\n",
    "  parse         Parse URL (using its spider) and print the results\n",
    "  runspider     Run a self-contained spider (without creating a project)\n",
    "  settings      Get settings values\n",
    "  shell         Interactive scraping console\n",
    "  startproject  Create new project\n",
    "  version       Print Scrapy version\n",
    "  view          Open URL in browser, as seen by Scrapy\n",
    "**除去全局命令之后，剩下的即为项目命令**  \n",
    "主要命令有：bench\\check\\crawl\\edit\\genspider\\list\\parse  \n",
    "**值得注意的是**  \n",
    "全局命令既可以在非scrapy项目文件夹中使用，也可以在项目文件夹中使用；而项目命令一般只能在scrapy项目文件夹中用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（1）bench命令**  \n",
    "使用此命令可以测试本地硬件的性能。\n",
    "命令：scrapy bench   \n",
    "根据输出结果可查看硬件性能，每分钟大概爬取多少个网页，但是实际情况不定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（2）Genspider命令**  \n",
    "可以使用genspider命令来创建scrapy爬虫文件，快速创建爬虫文件的方式。  \n",
    "使用该命令可以基于现有的爬虫模板直接生成一个新的爬虫文件，非常方便。  \n",
    "例子：scrapy genspider -l  \n",
    "可以看到当前可以使用的模板有：  \n",
    "Available templates:\n",
    "  basic\n",
    "  crawl\n",
    "  csvfeed\n",
    "  xmlfeed\n",
    "例子：scrapy genspider -t basic weisuen iqianyue.com  \n",
    "使用basic模板创建新的爬虫文件weisuen.py。  \n",
    "可以通过-d参数查看某个爬虫模板的内容，例如： scrapy genspider -d csvfeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)check命令**  \n",
    "爬虫测试比较麻烦，所以在scrapy中使用合同/契约（contract）的方式对爬虫进行测试。  \n",
    "例子：scrapy check weisuen (爬虫名，而不是爬虫文件名）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)Crawl命令**  \n",
    "可以通过crawl命令来启动某个爬虫，启动格式是“scrapy crawl爬虫名”  \n",
    "例子：scrapy crawl weisuen --loglevel=INFO  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)List命令**  \n",
    "通过scrapy中的list命令，可以列出当前可使用的爬虫文件  \n",
    "例如：进入myfirstpjt项目所在目录：scrapy list，可看到当前可使用的爬虫文件：weisuen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)Edit命令**  \n",
    "通过scrapy中的edit命令，可以直接打开对应编辑器对爬虫文件对爬虫文件进行编辑，该命令在Windows中执行会出现一点问题，一般会用python IDE（如PyCharm）直接对爬虫项目进行管理和编辑，该命令在Linux中很方便。   \n",
    "例如：scrapy list（查看文件）  \n",
    "scrapy edit abc（编辑文件abc）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（7）Parse命令**  \n",
    "实现获取指定URL网址，并使用对应的爬虫文件进行处理和分析。  \n",
    "比如，scrapy parse http://www.baidu.com --nolog 获取百度首页，这里没有指定爬虫文件和函数，所以会用默认的。  \n",
    "查看scrapy parse的参数： d:\\python\\crawler\\myfirstpjt>scrapy parse -h  \n",
    "Options\n",
    "_____________________________________________________________________________\n",
    "--help, -h              show this help message and exit\n",
    "--spider=SPIDER         use this spider without looking for one #指定爬虫文件\n",
    "-a NAME=VALUE           set spider argument (may be repeated) #设置spider参数\n",
    "--pipelines             process items through pipelines #处理items\n",
    "--nolinks               don't show links to follow (extracted requests) #不展示提取到的链接信息\n",
    "--noitems               don't show scraped items #不展示得到的items\n",
    "--nocolour              avoid using pygments to colorize the output #输出结果颜色不高亮\n",
    "--rules, -r             use CrawlSpider rules to discover the callback \n",
    "                        #使用CrawlSpider规则处理回调函数\n",
    "--callback=CALLBACK, -c CALLBACK\n",
    "                        use this callback for parsing, instead looking for a\n",
    "                        callback #使用spider中用于处理返回的响应的回调函数\n",
    "--meta=META, -m META    inject extra meta into the Request, it must be a valid\n",
    "                        raw json string\n",
    "--depth=DEPTH, -d DEPTH #设置爬行深度，默认深度为1\n",
    "                        maximum depth for parsing requests [default: 1]\n",
    "--verbose, -v           print each depth level one by one #显示每层的详细信息\n",
    "\n",
    "Global Options\n",
    "__________________________________________________________________\n",
    "--logfile=FILE          log file. if omitted stderr will be used\n",
    "--loglevel=LEVEL, -L LEVEL\n",
    "                        log level (default: DEBUG)\n",
    "--nolog                 disable logging completely\n",
    "--profile=FILE          write python cProfile stats to FILE\n",
    "--pidfile=FILE          write process ID to FILE\n",
    "--set=NAME=VALUE, -s NAME=VALUE\n",
    "                        set/override setting (may be repeated)\n",
    "--pdb                   enable pdb on failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战：Items的编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'email': '17888828903@163.com', 'job': 'student', 'name': 'yang'}\n"
     ]
    }
   ],
   "source": [
    "#理解Items\n",
    "import scrapy\n",
    "class person(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    job = scrapy.Field()\n",
    "    email = scrapy.Field()\n",
    "yang = person(name = \"yang\",job=\"student\",email=\"17888828903@163.com\")\n",
    "print(yang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'student'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yang[\"job\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ye'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yang['name']='ye'\n",
    "yang['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'job', 'email'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取字段名\n",
    "yang.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemsView({'email': '17888828903@163.com', 'job': 'student', 'name': 'ye'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取该对象中的项目视图（ItemView)\n",
    "yang.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战：Spider的编写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spider类是Scrapy中一个与爬虫相关的基类，所有爬虫文件必须继承该类（scrapy.Spider）。  \n",
    "爬虫文件：定义如何对网站进行相应的爬取，爬取动作及数据提取等操作都在此定义和编写。  \n",
    "例如：genspider命令创建一个爬虫文件，打开、修改和编写。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Xpath基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手写网络爬虫的时候，使用正则表达式对爬取到的数据进行筛选和提取，在scrapy会使用XPath表达式进行数据筛选和提取。  \n",
    "XPath是一种XML路径语言，通过该语言可以在XML文档中迅速地查找相应信息，XPath通常叫做XPath selector。   \n",
    "**(1)**在XPath表达式中，使用“/”可以选择某个标签，并且可以使用哦“/”进行多层标签的查找。  \n",
    "比如在html文件中查找，“/html/body/h2”查找标签/*<h2></h2>*/对应的内容。如果要获取该标签中的文本信息，通过text()实现。/html/body/h2/text()  \n",
    "**(2)**使用“//”可以提取某个标签的所有信息。比如上方代码中出现了多个<p>标签，如果要将所有<p>标签的所有信息都提取出来，可以使用“//”实现。如：//p。  \n",
    "**(3)**如果要获取所有属性X的值为Y的<z>标签的内容，可以通过//z[@X=\"Y\"]获取。  \n",
    "比如：//img[@class=\"f1\"]  \n",
    "**(4)**更多XPath的学习查看官方手册。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider类参数传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": "7",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
